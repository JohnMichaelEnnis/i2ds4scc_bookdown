[["index.html", "An Introduction to Data Science for Sensory and Consumer Scientists Preface", " An Introduction to Data Science for Sensory and Consumer Scientists John Ennis, Julien Delarue, and Thierry Worch 2020-12-30 Preface Welcome to the website for Introduction to Data Science for Sensory and Consumer Scientists. This book being written in the open and is currently under development. "],["about-the-authors.html", "About the Authors", " About the Authors John Ennis  Julien Delarue  Thierry Worch  "],["intro.html", "Chapter 1 Introduction 1.1 Core principles in Sensory and Consumer Science 1.2 How should sensory and consumer scientists learn data science? 1.3 Caution: Dont that everybody does 1.4 Example projects", " Chapter 1 Introduction Sensory and consumer science (SCS) is consider as a pillar of food science and technology and is useful to product development, quality control and market research. Most scientific and methodological advances in the field are applied to food. This book makes no exception as we chose a cookie formulation dataset as a main thread. However, SCS widely applies to many other consumer goods so are the content of this book and the principles set out below. 1.1 Core principles in Sensory and Consumer Science 1.1.1 Measuring and analyzing human responses Sensory and consumer science aims at measuring and understanding consumers sensory perceptions as well as the judgements, emotions and behaviors that may arise from these perceptions. SCS is thus primarily a science of measurement, although a very particular one that uses human beings and their senses as measuring instruments. In other words, sensory and consumer researchers measure and analyze human responses. To this end, SCS relies essentially on sensory evaluation which comprises a set of techniques that mostly derive from psychophysics and behavioral research. It uses psychological models to help separate signal from noise in collected data [ref OMahony, D.Ennis, others?]. Besides, sensory evaluation has developed its own methodological framework that includes most refined techniques for the accurate measurement of product sensory properties while minimizing the potentially biasing effects of brand identity and the influence of other external information on consumer perception [Lawless &amp; Heymann, 2010]. A detailed description of sensory methods is beyond the scope of this book and many textbooks on sensory evaluation methods are available to readers seeking more information. However, just to give a brief overview, it is worth remembering that sensory methods can be roughly divided into three categories, each of them bearing many variants: - Discrimination tests that aim at detecting subtle differences between two products. - Descriptive analysis (DA), also referred to as sensory profiling, aims at providing both qualitative and quantitative information about product sensory properties. - Hedonic tests. This category gathers affective tests that aim at measuring consumers liking for the tested products or their preferences among a product set. Each of these test categories generates its own type of data and related statistical questions in relation to the objectives of the study. Typically, data from difference tests consist in series of correct/failed binary answers depending on whether judges successfully picked the odd sample(s) among a set of three or more samples. These are used to determine whether the number of correct choices is above the level expected by chance. Conventional descriptive analysis data consist in intensity scores given by each panelist to evaluated samples on a series of sensory attributes, hence resulting in a product x attribute x panelist dataset (Figure 1). Note that depending on the DA method, quantifying means other than intensity ratings can be used (ranks, frequency, etc.). Most frequently, each panelist evaluates all the samples in the product set. However, the use of balanced incomplete design can also be found when the experimenters aim to limit the number of samples evaluated by each subject. Eventually, hedonic test datasets consist in hedonic scores (ratings for consumers degree of liking or preference ranks) given by each interviewed consumer to a series of products. As for DA, each consumer usually evaluates all the samples in the product set, but balanced incomplete designs are sometimes used too. In addition, some companies favor pure monadic evaluation of product (i.e. between-subject design or independent groups design) which obviously result in unrelated sample datasets. Sensory and consumer researchers also borrow methods from other fields, in particular from sociology and experimental psychology. Definitely a multidisciplinary area, SCS develops in many directions and reaches disciplines that range from genetics and physiology to social marketing, behavioral economics and computational neuroscience. So have diversified the types of data sensory and consumer scientists must deal with. 1.2 How should sensory and consumer scientists learn data science? 1.3 Caution: Dont that everybody does 1.4 Example projects "],["data-science.html", "Chapter 2 What is Data Science? 2.1 History and Definition 2.2 Workflow 2.3 Benefits of Data Science 2.4 How to Learn Data Science 2.5 How to Use This Book 2.6 Common Data Science Tools", " Chapter 2 What is Data Science? In this chapter we explain what is data science. 2.1 History and Definition Data science has been called the sexiest job of the 21st century by Harvard Business Review [insert DJ Patil reference], but what is it? As with all rapidly growing fields, the definition depends on who you ask. Before we give our definition, however, we provide a brief history for context. To begin, we note that there was a movement in early computer science to call their field data science. Chief among the advocates for this viewpoint was Peter Naur, winner of the 2005 Turing award. This viewpoint is detailed in the preface to his 1974 book, Concise Survey of Computer Methods, where he states that data science is the science of dealing with data, once they have been established. From his perspective, this is the purpose of computer science. This viewpoint is echoed in the statement, often attributed to Edsger Dijkstr, that Computer science is no more about computers than astronomy is about telescopes. Interestingly, a similar movement arose in statistics, starting in 1962 with John Tukeys statements that Data analysis, and the parts of statistics which adhere to it, must  take on the characteristics of science rather than those of mathematics and that data analysis is intrinsically an empirical science. This movement culminated in 1997 when Jeff Wu proposed during his inaugural lecture, upon becoming the chair of the University of Michigans statistics department, that statistics should be called data science. These two movements came together in 2001 in William S. Clevelands paper Data Science: An Action Plan for Expanding the Technical Areas in the Field of Statistics. In this highly influential monograph, Cleveland makes the key assertion that The value of technical work is judged by the extent ot which it benefits the data analyst, either directly or indirectly. [FOOTNOTE: It is worth noting that these two movements were connected by substantial work in the areas of statistical computing, knowledge discovery, and data mining, with important work contributed by Gregory Piatetsky-Shapiro, Usama Fayyad, and Padhraic Smyth among many others.] Putting this history together, we provide our definition of data science as: The intersection of statistics, computer science, and industrial design. Accordingly, we use the following three definitions of these fields: Statistics: The branch of mathematics dealing with the collection, analysis, interpretation, and presentation of masses of numerical data. Computer Science: Computer science is the study of processes that interact with data and that can be represented as data in the form of programs. Industrial Design: The professional service of creating and developing concepts and specifications that optimize the function, value, and appearance of products and systems for the mutual benefit of both user and manufacturer. Hence data science is the communication of value through the collection, processing, analysis, and interpretation of data. 2.2 Workflow A schematic of a data scientific workflow is shown in Figure 2.1. Each section is described in greater detail below. Figure 2.1: Data scientific workflow. 2.2.1 Data Preparation 2.2.1.1 Inspect Goal: Gain familiarity with the data Key Steps: Learn collection details Check data imported correctly Determine data types Ascertain consistency and validity Tabulate and compute other basic summary statistics Create basic plots of key variables of interest 2.2.1.2 Clean Goal: Prepare data for analysis Key Steps: Remove/correct errors Make data formatting consistent Organize text data Create tidy data (one observation per row) Organize data into related tables Document all choices 2.2.2 Data Analysis 2.2.2.1 Transform Goal: Adjust data as needed for analysis Key Steps: Create secondary variables Decorrelate data Identify latent factors Engineer new features 2.2.2.2 Explore Goal: Allow data to suggest hypotheses Key Steps: Graphical visualizations Exploratory analyses Note: Caution must be taken to avoid high false discovery rate when using automated tools 2.2.2.3 Model Goal: Conduct formal statistical modeling Key Steps: Conduct traditional statistical modeling Build predictive models Note: This step may feed back into transform and explore 2.2.3 Insight Delivery 2.2.3.1 Communicate Goal: Exchange research information Key Steps: Automate reporting as much as possible Share insights Receive feedback Note: Design principles essential to make information accessible 2.2.3.2 Reformulate Goal: Incorporate feedback into workflow Key Steps: Investigate new questions Revise communications Note: Reformulation make take us back to data cleaning 2.3 Benefits of Data Science 2.3.1 Reproducible Research Time savings Collaboration Continuous improvement 2.3.2 Data-Driven Decision Making 2.3.3 Standardized Data Collection 2.3.4 Standardized Reporting Especially valuable when there are multiple sites globally 2.3.5 Improved Business Impact 2.4 How to Learn Data Science Learning data science is much like learning a language or learning to play an instrument - you have to practice. Our advice based on mentoring many students and clients is to get started sooner rather than later, and to accept that the code youll write in the future will always be better than the code youll write today. Also, many of the small details that separate an proficient data scientist from a novice can only really be learned through practice as there are too many small details to learn them all in advice. So, starting today, do your best to write at least some code for all your projects. If a time deadline prevents you from completing the analysis in R, thats fine, but at least gain the experience of making an RStudio project and loading the data in R. Then, as time allows, try to duplicate your analyses in R, being quick to search for solutions when you run into errors. Often simply copying and pasting your error into a search engine will be enough to find the solution to your problem. Moreover, searching for solutions is its own skill that also requires practice. Finally, if you are really stuck, reach out to a colleague (or even the authors of this book) for help 2.5 How to Use This Book We recommend following the instructions in Chapter 4 to get started. 2.6 Common Data Science Tools We continue our discussion of getting started with R in the next chapter. "],["reproducible-research.html", "Chapter 3 Reproducible Research 3.1 Principles 3.2 Tools 3.3 Documentation 3.4 Version control 3.5 Online repositories for team collaboration 3.6 Building a code base", " Chapter 3 Reproducible Research 3.1 Principles 3.2 Tools 3.2.1 GitHub 3.2.2 R scripts 3.2.3 RMarkdown 3.2.4 Shiny 3.3 Documentation 3.4 Version control 3.5 Online repositories for team collaboration 3.6 Building a code base 3.6.1 Internal functions 3.6.2 Packages "],["start-R.html", "Chapter 4 Getting Started with R 4.1 R 4.2 Why R? 4.3 RStudio 4.4 Git and GitHub 4.5 RAW MATERIAL", " Chapter 4 Getting Started with R 4.1 R R is an open-source programming language and software environment First released in 1995, R is an open-source implementation of S R was developed by Ross Ihaka and Robert Gentleman The name R is partly a play on Ihakas and Gentlemans first names R is a scripting language (not a compiled language) Lines of R code run (mostly) in order R is currently the 7th most popular programming language in the world 4.1.1 Why Learn a Programming Language? Control Speed Reduced errors Increased capability Continuous improvement Improved collaboration Reproducible results 4.1.2 Why R? R originated as a statistical computing language It has a culture germane to sensory science R is well-supported with an active community Extensive online help is available Many books, courses, and other educational material exist The universe of available packages is vast R excels at data manipulation and results reporting R has more specialized tools for sensory analysis than other programming language 4.2 Why R? For sensory and consumer scientists, we recommend the R ecosystem of tools for three main reasons. The first reason is cultural - R has from its inception been oriented more towards statistics than to computer science, making the feeling of programming in R more natural (in our experience) for sensory and consumer scientists than programming in Python. This opinion of experience is not to say that a sensory and consumer scientist shouldnt learn Python if they are so inclined, or even that Python tools arent sometimes superior to R tools (in fact, they sometimes are). This latter point leads to our second reason, which is that R tools are typically better suited to sensory and consumer science than are Python tools. Even when Python tools are superior, the R tools are still sufficient for sensory and consumer science purposes, plus there are many custom packages such as SensR, SensoMineR, and FactorMineR that have been specifically developed for sensory and consumer science. Finally, the recent work by the RStudio company, and especially the exceptional work of Hadley Wickham, has lead to a very low barrier to entry for programming within R together with exceptional tools for data manipulation. 4.2.1 Steps to Install R The first step in this journey is to install R. For this, visit The R Project for Statistical Computing. From there, follow the download instructions to install R for your particular platform. https://cran.r-project.org/bin/windows/base/ Download the latest version of R Install R with default options You will almost certainly be running 64-bit R Note: If you are running R 4.0 or higher, you might need to install Rtools: https://cran.r-project.org/bin/windows/Rtools/ 4.3 RStudio 4.3.1 Steps to Install RStudio Next you need to install RStudio, which is our recommended integrated development environment (IDE) for developing R code. To do so, visit the RStudio desktop download page and follow the installation instructions. Once you have installed R and RStudio, you should be able to open RStudio and enter the following into the Console to receive the number 3 as your output: x &lt;- 1 y &lt;- 2 x + y ## [1] 3 Some recommendations upon installing RStudio: Change the color scheme to dark. Put the console on the right. https://www.rstudio.com/products/rstudio/download/#download Download and install the latest (almost certainly 64-bit) version of RStudio with default options Adjustments: Uncheck Restore .RData into workspace at startup Select Never for Save workspace to .RData on exit Change color scheme to dark (e.g. Idle Fingers) Put console on right 4.3.2 Create a Local Project Always work in an RStudio project Projects keep your files (and activity) organized Projects help manage your file path (so your computer can find things) Projects allow for more advanced capabilities later (like GitHub or renv) We cover the use of GitHub in a future webinar For now we create projects locally 4.3.3 Install and Load Packages As you use R, you will want to make use of the many packages others (and perhaps you) have written Essential packages (or collections): tidyverse, readxl Custom Microsoft office document creation officer, flextable, rvg, openxlsx, extrafont, extrafontdb Sensory specific packages sensR , SensoMineR, FactoMineR, factoextra There are many more, for statistical tests of all varieties, to multivariate analysis, to machine learning, to text analysis, etc. You only need to install each package once per R version To install a package, you can: Type install.packages([package name]) Use the RStudio dropdown In addition, if a script loads package that are not installed, RStudio will prompt you to install the package Notes: If you do not have write access on your computer, you might need IT help to install packages You might need to safelist various R related tools and sites 4.3.4 Run Sample Code Like any language, R is best learned first through example then through study We start with a series of examples to illustrate basic principles For this example, we analyze a series of Tetrad tests Suppose you have 15 out of 44 correct in a Tetrad test Using sensR, its easy to analyze these data: library(sensR) num_correct &lt;- 15 num_total &lt;- 44 discrim_res &lt;- discrim(num_correct, num_total, method = &quot;tetrad&quot;) print(discrim_res) ## ## Estimates for the tetrad discrimination protocol with 15 correct ## answers in 44 trials. One-sided p-value and 95 % two-sided confidence ## intervals are based on the &#39;exact&#39; binomial test. ## ## Estimate Std. Error Lower Upper ## pc 0.34091 0.07146 0.3333 0.4992 ## pd 0.01136 0.10719 0.0000 0.2488 ## d-prime 0.20363 0.96585 0.0000 1.0193 ## ## Result of difference test: ## &#39;exact&#39; binomial test: p-value = 0.5141 ## Alternative hypothesis: d-prime is greater than 0 4.4 Git and GitHub Git is a version control system that allows you to revert to earlier versions of your code, if necessary. GitHub is service that allows for online backups of your code and which facilitates collaboration between team members. We highly recommend that you integrate both Git and GitHub into your data scientific workflow. For a full review of Git and GitHub from an R programming perspective, we recommend Happy Git with R by Jenny Bryant. In what follows, we simply provide the minimum information needed to get you up and running with Git and GitHub. Also, for an insightful discussion of the need for version control, please see ((bryan2018excuse?)). 4.4.1 Git Install Git Windows macOS Register with RStudio 4.4.2 GitHub Create a GitHub account Register with RStudio 4.5 RAW MATERIAL 4.5.1 Principles 4.5.2 Tools 4.5.2.1 GitHub 4.5.2.2 R scripts 4.5.2.3 RMarkdown 4.5.2.4 Shiny 4.5.3 Documentation 4.5.4 Version control 4.5.5 Online repositories for team collaboration 4.5.6 Building a code base 4.5.6.1 Internal functions 4.5.6.2 Packages "],["data-collection.html", "Chapter 5 Data Collection 5.1 Background 5.2 Other details 5.3 Conclusions?", " Chapter 5 Data Collection 5.1 Background 5.2 Other details 5.3 Conclusions? "],["data-prep.html", "Chapter 6 Data Preparation 6.1 Data Importation 6.2 Data Inspection 6.3 Data Organization 6.4 Data Manipulation 6.5 Cleaning and Quality Assessment", " Chapter 6 Data Preparation To analyze data, one need data. If this data is already available in R, then the analysis can be performed directly. However, in much cases, the data is stored outside the R environment, and needs to be imported. 6.1 Data Importation In practice, the data might be stored in as many format as one can imagine, whether it ends up being a fairly common solution (.txt file, .csv file, or .xls/.xlsx file), or software specific (e.g. Stata, SPSS, etc.). Since it is very common to store the data in Excel spreadsheets (.xlsx) due to its simplicity, the emphasis is on this solution. Fortunately, most generalities presented for Excel files also apply to other formats through base::read.table() for .txt files, base::read.csv() and base::read.csv2() for .csv files, or through the {read} package (which is part of the {tidyverse}). For other (less common) formats, the reader can find packages that would allow importing their files into R. Particular interest can be given to the package {rio} (rio stands for R Input and Output) which provides an easy solution that 1. can handle a large variety of files, 2. can actually guess the type of file it is, and 3. provides tools to import, export, and convert almost any type of data format, including .csv, .xls and .xlsx, or data from other statistical software such as SAS (.sas7bdat and .xpt), SPSS (.sav and .por), or Stata (.dta). As an alternative, the package {foreign} provides functions that allow importing data stored from other statistical software (incl. Minitab, S, SAS, Stata, SPSS, etc.) Although Excel is most likely one of the most popular way of storing data, there are no {base} functions that allow importing such files easily. Fortunately, many packages have been developed in that purpose, including {XLConnect}, {xlsx}, {gdata}, and {readxl}. Due to its convenience and speed of execution, we will be using {readxl} here. 6.1.1 Importing Structured Excel File First, lets import the Sensory Profile.xlsx workbook using the readxl::read_xlsx() file, by informing as parameter the location of the file (informed in file_path using the package {here}) and the sheet we want to read from. This file is called structured as all the relevant information is already stored in the same sheet in a structured way. In other words, no decoding is required here, and there are no unexpected rows or columns (e.g. empty lines, or lines with additional information regarding the data but that is not data): The first row within the Data sheet of Sensory Profile.xlsx contains the headers, From the second row onwards, only data is being stored. Since this data will be used for some analyses, it is assigned data to an R object called sensory. library(here) ## here() starts at C:/Aigora/books/intro to data science/i2ds4scc_bookdown file_path &lt;- here(&quot;data&quot;,&quot;Sensory Profile.xlsx&quot;) library(readxl) sensory &lt;- read_xlsx(file_path, sheet=&quot;Data&quot;) To ensure that the importation went well, we print sensory to see how it looks like. Since {readxl} has been developed by Hadley Wickham and colleagues, its functions follow the {tidyverse} principles and the dataset thus imported is a tibble. Lets take advantage of the printing properties of a tibble to evaluate sensory: sensory ## # A tibble: 99 x 35 ## Judge Code Product Shiny `External color~ `Color evenness` `Qtt of inclusi~ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 J01 B 12GP_f 48.6 30 13.2 10.8 ## 2 J01 D 12GP16~ 46.2 45.6 37.8 0 ## 3 J01 C 12GP23~ 48 45.6 17.4 7.8 ## 4 J01 G 12SA_f 5.4 6.6 17.4 0 ## 5 J01 I 12SA23~ 0 42.6 18 21 ## 6 J01 E 16PPK_p 0 23.4 49.2 0 ## 7 J01 New 23pK_p 4.8 33.6 15.6 32.4 ## 8 J01 H 23PLK_p 0 51.6 48.6 23.4 ## 9 J01 F 29PPK_p 0 50.4 24 27.6 ## 10 J01 A ck1 52.8 30 22.8 9.6 ## # ... with 89 more rows, and 28 more variables: `Surface defects` &lt;dbl&gt;, `Print ## # quality` &lt;dbl&gt;, Thickness &lt;dbl&gt;, `Color constrast` &lt;dbl&gt;, `Overall odor ## # intensity` &lt;dbl&gt;, `Fatty odor` &lt;dbl&gt;, `Roasted odor` &lt;dbl&gt;, `Cereal ## # flavor` &lt;dbl&gt;, `Raw dough flavor` &lt;dbl&gt;, `Fatty flavor` &lt;dbl&gt;, `Dairy ## # flavor` &lt;dbl&gt;, `Roasted flavor` &lt;dbl&gt;, `Overall flavor persistence` &lt;dbl&gt;, ## # Salty &lt;dbl&gt;, Sweet &lt;dbl&gt;, Sour &lt;dbl&gt;, Bitter &lt;dbl&gt;, Astringent &lt;dbl&gt;, ## # Warming &lt;dbl&gt;, `Initial hardness` &lt;dbl&gt;, Brittle &lt;dbl&gt;, Crunchy &lt;dbl&gt;, ## # `Fatty in mouth` &lt;dbl&gt;, Light &lt;dbl&gt;, `Dry in mouth` &lt;dbl&gt;, `Qtt of ## # inclusions in mouth` &lt;dbl&gt;, Sticky &lt;dbl&gt;, Melting &lt;dbl&gt; sensory is a tibble with 99 rows and 35 columns that includes the Judge information (first column, defined as character), the Product information (second and third columns, defined as character), and the sensory attributes (fourth column onward, defined as numerical or dbl). Additionally, we can also print a summary() of sensory to get some extra information regarding the data (such as the minimum, maximum, mean and median for each numerical variable)\" summary(sensory) ## Judge Code Product Shiny ## Length:99 Length:99 Length:99 Min. : 0.0 ## Class :character Class :character Class :character 1st Qu.: 9.3 ## Mode :character Mode :character Mode :character Median :21.0 ## Mean :23.9 ## 3rd Qu.:38.4 ## Max. :54.0 ## External color intensity Color evenness Qtt of inclusions Surface defects ## Min. : 6.60 Min. : 6.60 Min. : 0.00 Min. : 4.80 ## 1st Qu.:27.00 1st Qu.:19.50 1st Qu.:13.80 1st Qu.:15.30 ## Median :34.80 Median :26.40 Median :19.80 Median :21.00 ## Mean :33.68 Mean :28.19 Mean :20.63 Mean :23.35 ## 3rd Qu.:42.60 3rd Qu.:37.20 3rd Qu.:29.10 3rd Qu.:30.60 ## Max. :55.20 Max. :53.40 Max. :40.80 Max. :51.60 ## Print quality Thickness Color constrast Overall odor intensity ## Min. :12.00 Min. : 7.80 Min. : 5.40 Min. : 0.00 ## 1st Qu.:36.30 1st Qu.:18.30 1st Qu.:21.00 1st Qu.:10.20 ## Median :40.80 Median :25.80 Median :32.40 Median :18.00 ## Mean :40.72 Mean :25.48 Mean :30.02 Mean :18.67 ## 3rd Qu.:47.10 3rd Qu.:32.10 3rd Qu.:40.20 3rd Qu.:26.10 ## Max. :60.00 Max. :52.80 Max. :51.60 Max. :40.20 ## Fatty odor Roasted odor Cereal flavor Raw dough flavor ## Min. : 0.00 Min. : 0.00 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 0.00 1st Qu.: 8.10 1st Qu.:18.00 1st Qu.: 3.00 ## Median : 5.40 Median :15.00 Median :25.20 Median :13.20 ## Mean : 6.81 Mean :15.07 Mean :24.99 Mean :14.23 ## 3rd Qu.:10.50 3rd Qu.:20.70 3rd Qu.:31.20 3rd Qu.:24.60 ## Max. :27.00 Max. :42.00 Max. :48.00 Max. :43.20 ## Fatty flavor Dairy flavor Roasted flavor Overall flavor persistence ## Min. : 0.000 Min. : 0.000 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.: 9.00 1st Qu.:16.20 ## Median : 6.600 Median : 7.200 Median :17.40 Median :22.80 ## Mean : 7.419 Mean : 9.106 Mean :17.68 Mean :22.73 ## 3rd Qu.:13.200 3rd Qu.:13.500 3rd Qu.:24.60 3rd Qu.:28.80 ## Max. :24.000 Max. :46.800 Max. :51.60 Max. :43.80 ## Salty Sweet Sour Bitter ## Min. : 0.000 Min. : 0.00 Min. : 0.000 Min. : 0.000 ## 1st Qu.: 0.000 1st Qu.: 9.90 1st Qu.: 0.000 1st Qu.: 0.000 ## Median : 1.200 Median :18.00 Median : 0.000 Median : 7.800 ## Mean : 5.027 Mean :17.82 Mean : 1.461 Mean : 8.103 ## 3rd Qu.:10.050 3rd Qu.:24.30 3rd Qu.: 0.000 3rd Qu.:14.100 ## Max. :19.200 Max. :43.20 Max. :13.800 Max. :27.600 ## Astringent Warming Initial hardness Brittle ## Min. : 0.00 Min. : 0.00 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 0.00 1st Qu.: 9.30 1st Qu.:19.50 1st Qu.:27.60 ## Median : 8.40 Median :16.80 Median :30.60 Median :34.80 ## Mean :11.45 Mean :16.76 Mean :30.12 Mean :31.77 ## 3rd Qu.:19.50 3rd Qu.:25.20 3rd Qu.:39.30 3rd Qu.:39.14 ## Max. :34.20 Max. :47.40 Max. :60.00 Max. :57.00 ## Crunchy Fatty in mouth Light Dry in mouth ## Min. : 8.40 Min. : 0.00 Min. : 5.40 Min. :11.40 ## 1st Qu.:23.25 1st Qu.: 0.00 1st Qu.:22.80 1st Qu.:39.00 ## Median :30.60 Median : 5.40 Median :31.80 Median :45.00 ## Mean :29.62 Mean : 7.32 Mean :30.21 Mean :42.99 ## 3rd Qu.:36.30 3rd Qu.:12.90 3rd Qu.:37.20 3rd Qu.:49.80 ## Max. :48.60 Max. :27.00 Max. :53.40 Max. :58.80 ## Qtt of inclusions in mouth Sticky Melting ## Min. : 0.00 Min. : 6.00 Min. : 0.0 ## 1st Qu.:15.90 1st Qu.:27.00 1st Qu.:13.2 ## Median :26.40 Median :33.60 Median :19.2 ## Mean :24.92 Mean :32.73 Mean :20.5 ## 3rd Qu.:35.40 3rd Qu.:39.60 3rd Qu.:27.3 ## Max. :45.60 Max. :52.80 Max. :38.4 At last, we can list the structure of the dataset through the str() function. str(sensory) ## tibble [99 x 35] (S3: tbl_df/tbl/data.frame) ## $ Judge : chr [1:99] &quot;J01&quot; &quot;J01&quot; &quot;J01&quot; &quot;J01&quot; ... ## $ Code : chr [1:99] &quot;B&quot; &quot;D&quot; &quot;C&quot; &quot;G&quot; ... ## $ Product : chr [1:99] &quot;12GP_f&quot; &quot;12GP16PSL_m&quot; &quot;12GP23P_m&quot; &quot;12SA_f&quot; ... ## $ Shiny : num [1:99] 48.6 46.2 48 5.4 0 0 4.8 0 0 52.8 ... ## $ External color intensity : num [1:99] 30 45.6 45.6 6.6 42.6 23.4 33.6 51.6 50.4 30 ... ## $ Color evenness : num [1:99] 13.2 37.8 17.4 17.4 18 49.2 15.6 48.6 24 22.8 ... ## $ Qtt of inclusions : num [1:99] 10.8 0 7.8 0 21 0 32.4 23.4 27.6 9.6 ... ## $ Surface defects : num [1:99] 13.2 48.6 14.4 36 36 12.6 13.8 18 39.6 22.8 ... ## $ Print quality : num [1:99] 54 45.6 49.2 42.6 51 47.4 43.8 45.6 53.4 48.6 ... ## $ Thickness : num [1:99] 35.4 43.2 25.8 32.4 31.8 29.4 36 31.2 36 38.4 ... ## $ Color constrast : num [1:99] 40.2 45.6 17.4 41.4 41.4 12.6 36 5.4 21 37.8 ... ## $ Overall odor intensity : num [1:99] 24.6 7.2 21.6 13.8 26.4 18 16.2 13.8 0 16.8 ... ## $ Fatty odor : num [1:99] 5.4 0 0 0 6.6 8.4 7.8 7.8 0 6.6 ... ## $ Roasted odor : num [1:99] 20.4 6 18.6 16.2 16.8 16.2 16.2 12.6 7.2 15.6 ... ## $ Cereal flavor : num [1:99] 25.8 16.2 30 18 28.8 21.6 23.4 23.4 28.8 24.6 ... ## $ Raw dough flavor : num [1:99] 28.8 28.2 26.4 21 26.4 27.6 31.2 27 27.6 28.2 ... ## $ Fatty flavor : num [1:99] 7.2 0 0 6 7.2 6.6 10.8 7.2 0 13.8 ... ## $ Dairy flavor : num [1:99] 0 0 0 0 0 0 4.8 0 0 0 ... ## $ Roasted flavor : num [1:99] 19.2 28.2 27 21.6 25.8 20.4 26.4 24.6 22.2 24.6 ... ## $ Overall flavor persistence: num [1:99] 24.6 14.4 25.2 18 22.8 21 24 21.6 25.2 23.4 ... ## $ Salty : num [1:99] 0 0 0 0 0 0 3.6 0 0 0 ... ## $ Sweet : num [1:99] 19.2 11.4 9.6 10.8 21 20.4 21 10.2 21 13.8 ... ## $ Sour : num [1:99] 0 0 0 0 0 0 0.6 0 0 0 ... ## $ Bitter : num [1:99] 21.6 9 21 0 22.8 8.4 27.6 9.6 18.6 19.2 ... ## $ Astringent : num [1:99] 0 18.6 25.8 21 26.4 16.2 31.8 23.4 25.2 0 ... ## $ Warming : num [1:99] 0 0 13.8 10.8 15 0 27.6 19.8 14.4 0 ... ## $ Initial hardness : num [1:99] 17.4 19.8 33 10.2 29.4 16.2 18 34.2 28.2 11.4 ... ## $ Brittle : num [1:99] 35.4 33.6 27.6 29.4 34.8 38.4 35.4 35.4 33 39.6 ... ## $ Crunchy : num [1:99] 32.4 28.8 25.2 27 32.4 35.4 34.8 30.6 27.6 25.8 ... ## $ Fatty in mouth : num [1:99] 9.6 9.6 6 5.4 12 7.2 14.4 5.4 0 0 ... ## $ Light : num [1:99] 21 40.5 20.4 25.8 21 ... ## $ Dry in mouth : num [1:99] 25.8 28.2 31.2 22.2 27.6 34.2 31.8 37.8 33.6 27 ... ## $ Qtt of inclusions in mouth: num [1:99] 22.2 13.2 10.2 9 29.4 18 31.2 25.8 26.4 27.6 ... ## $ Sticky : num [1:99] 35.4 21.6 37.2 22.8 37.2 39 34.2 36.6 36 37.2 ... ## $ Melting : num [1:99] 36 34.2 8.4 34.8 19.8 34.8 36.6 19.2 21.6 33.6 ... This function provides an overview of the first element of each variables (and the format they are in) as a list, and allows the user to have a first scan to eventually detect errors or mishaps during the importation. Here, the data has been imported as expected. 6.1.2 Importing Unstructured Excel File In some cases, the dataset is not so well organized/structured, and may need to be decoded. This is the case for the workbook entitled TFEQ.xlsx. For this file: The variables name have been coded and their corresponding names (together with some other valuable information we will be using in the next chapter) are stored in a different sheet entitled Variables; The different levels of each variable (including their code and corresponding names) are stored in another sheet entitled Levels. To import and decode this dataset, multiple steps are required: Import the variables name only; Import the information regarding the levels; Import the dataset without the first line of header, but by providing the correct names obtained in the first step; Decode each question (when needed) by replacing the numerical code by their corresponding labels. Lets start with importing the variables names from TFEQ.xlsx (sheet Variables) file_path &lt;- here(&quot;data&quot;,&quot;TFEQ.xlsx&quot;) var_names &lt;- read_xlsx(file_path, sheet=&quot;Variables&quot;) var_names ## # A tibble: 62 x 6 ## Code `Original Questions (French)` Name Direction Value `Full Question` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Q1 Où habitez-vous? Living ~ &lt;NA&gt; NA &lt;NA&gt; ## 2 Q2 Comment habitez-vous? Housing &lt;NA&gt; NA &lt;NA&gt; ## 3 Q3 N° Juge Judge &lt;NA&gt; NA &lt;NA&gt; ## 4 Q4 Quelle est votre taille (en m~ Height &lt;NA&gt; NA &lt;NA&gt; ## 5 Q5 Quel est votre poids? Weight &lt;NA&gt; NA &lt;NA&gt; ## 6 Q6 IMC BMI &lt;NA&gt; NA &lt;NA&gt; ## 7 Q7 Quelle est votre situation ma~ Marital~ &lt;NA&gt; NA &lt;NA&gt; ## 8 Q8 Nombre de personnes vivant au~ Househo~ &lt;NA&gt; NA &lt;NA&gt; ## 9 Q9 Dans quelle tranche de revenu~ Income ~ &lt;NA&gt; NA &lt;NA&gt; ## 10 Q10 Dans quelle catégorie sociopr~ Occupat~ &lt;NA&gt; NA &lt;NA&gt; ## # ... with 52 more rows In a similar way, lets import the information related to the levels of each variable, stored in the Levels sheet. A deeper look at the Levels sheet shows that only the coded names of the variables are available. In order to include the final names, var_names is joined (using inner_join). library(tidyverse) ## -- Attaching packages --------------------------------------- tidyverse 1.3.0 -- ## v ggplot2 3.3.2 v purrr 0.3.4 ## v tibble 3.0.4 v dplyr 1.0.2 ## v tidyr 1.1.2 v stringr 1.4.0 ## v readr 1.4.0 v forcats 0.5.0 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() var_labels &lt;- read_xlsx(file_path, sheet=&quot;Levels&quot;) %&gt;% inner_join(dplyr::select(var_names, Code, Name), by=c(Question=&quot;Code&quot;)) var_labels ## # A tibble: 172 x 5 ## Question Code `Original Levels (French)` Levels Name ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Q1 1 Zone urbaine Urban Area Living area ## 2 Q1 2 Zone rurbaine Rurban Area Living area ## 3 Q1 3 Zone rurale Rural Area Living area ## 4 Q2 1 Appartement Apartment Housing ## 5 Q2 2 Maison individuelle House Housing ## 6 Q7 1 Divorcé(e) Divorced Marital status ## 7 Q7 2 Marié(e) Married Marital status ## 8 Q7 3 Vivant maritalement Conjugal Marital status ## 9 Q7 4 Célibataire Single Marital status ## 10 Q7 5 Pacsé(e) Civil Partnership Marital status ## # ... with 162 more rows Note: In some cases, this information is directly available in the dataset as sub-header: A solution is then to import the first rows of the dataset that contain this information using the parameter n_max from `readxl::read_xlsx``. For each variable (when information is available), store that information as a list of tables that contains the code and their corresponding label. THIS SECTION BELOW MIGHT NEED TO GET PASSED ON TO THE EXERCISE Since most likely this system of coding follow a fixed pattern, we strongly recommend the use of {tidytext} and its function unnest_tokens(). For example, lets imagine that the our information is structured as code1=label1,code2=label2, (e.g. 0=No,1=Yes). In such case, first use unnest_tokens() to split this string by , This creates a tibble with as many rows as there are code=label and one column. Next, split this column into two columns using separate() and sep=\"=\". (PREVIOUS PART) TO BE GIVEN AS AN EXAMPLE/EXERCISE Finally, we import the dataset (Data) by substituting the coded names with their real names. To do so, we skip reading the first line (skip=1) that contains the coded names, and force the column names to be defined by Name from var_names (after ensuring that the names sequence perfectly match across the two tables!). TFEQ_data &lt;- read_xlsx(file_path, sheet=&quot;Data&quot;, col_names=var_names$Name, skip=1) summary(TFEQ_data) ## Living area Housing Judge Height ## Min. :1.000 Min. :1.000 Length:107 Min. :1.450 ## 1st Qu.:1.000 1st Qu.:1.000 Class :character 1st Qu.:1.600 ## Median :1.000 Median :2.000 Mode :character Median :1.630 ## Mean :1.542 Mean :1.682 Mean :1.634 ## 3rd Qu.:2.000 3rd Qu.:2.000 3rd Qu.:1.680 ## Max. :3.000 Max. :2.000 Max. :1.800 ## Weight BMI Marital status Household size ## Min. :43.00 Min. :17.63 Min. :1.000 Min. :0.000 ## 1st Qu.:53.50 1st Qu.:20.30 1st Qu.:2.000 1st Qu.:2.000 ## Median :58.00 Median :21.63 Median :3.000 Median :3.000 ## Mean :58.25 Mean :21.79 Mean :2.944 Mean :2.944 ## 3rd Qu.:62.35 3rd Qu.:23.33 3rd Qu.:4.000 3rd Qu.:4.000 ## Max. :82.00 Max. :26.47 Max. :7.000 Max. :6.000 ## Income range Occupation Highest degree D1 ## Min. :1.000 Min. : 1.000 Min. :1.00 Min. :0.000 ## 1st Qu.:2.000 1st Qu.: 2.000 1st Qu.:2.00 1st Qu.:0.000 ## Median :2.000 Median : 6.000 Median :3.00 Median :0.000 ## Mean :2.421 Mean : 6.486 Mean :2.71 Mean :0.215 ## 3rd Qu.:3.000 3rd Qu.: 9.000 3rd Qu.:4.00 3rd Qu.:0.000 ## Max. :4.000 Max. :17.000 Max. :6.00 Max. :1.000 ## D2 H1 R1 H2 ## Min. :0.0000 Min. :0.000 Min. :0.0000 Min. :0.0000 ## 1st Qu.:0.0000 1st Qu.:0.000 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :1.0000 Median :0.000 Median :0.0000 Median :1.0000 ## Mean :0.5794 Mean :0.243 Mean :0.4579 Mean :0.5514 ## 3rd Qu.:1.0000 3rd Qu.:0.000 3rd Qu.:1.0000 3rd Qu.:1.0000 ## Max. :1.0000 Max. :1.000 Max. :1.0000 Max. :1.0000 ## R2 D3 H3 D4 ## Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :0.0000 ## 1st Qu.:0.0000 1st Qu.:0.5000 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :0.0000 Median :1.0000 Median :0.0000 Median :1.0000 ## Mean :0.2991 Mean :0.7477 Mean :0.3738 Mean :0.5607 ## 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 ## Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 ## R3 D5 H4 D6 ## Min. :0.0000 Min. :0.0000 Min. :0.000 Min. :0.0000 ## 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:0.000 1st Qu.:0.0000 ## Median :1.0000 Median :0.0000 Median :0.000 Median :0.0000 ## Mean :0.5327 Mean :0.3551 Mean :0.486 Mean :0.3458 ## 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.000 3rd Qu.:1.0000 ## Max. :1.0000 Max. :1.0000 Max. :1.000 Max. :1.0000 ## R4 D7 D8 H5 ## Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :0.0000 ## 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :0.0000 Median :0.0000 Median :1.0000 Median :1.0000 ## Mean :0.4486 Mean :0.3178 Mean :0.5047 Mean :0.6168 ## 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 ## Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 ## R5 H6 D9 R6 ## Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :0.0000 ## 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :0.0000 Median :1.0000 Median :1.0000 Median :1.0000 ## Mean :0.4486 Mean :0.5888 Mean :0.5701 Mean :0.5421 ## 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 ## Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 ## H7 R7 H8 D10 ## Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :0.0000 ## 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :0.0000 Median :0.0000 Median :0.0000 Median :0.0000 ## Mean :0.3271 Mean :0.2523 Mean :0.2243 Mean :0.4206 ## 3rd Qu.:1.0000 3rd Qu.:0.5000 3rd Qu.:0.0000 3rd Qu.:1.0000 ## Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 ## H9 D11 R8 H10 ## Min. :0.0000 Min. :0.000 Min. :0.0000 Min. :0.000 ## 1st Qu.:0.0000 1st Qu.:0.000 1st Qu.:0.0000 1st Qu.:0.000 ## Median :0.0000 Median :0.000 Median :0.0000 Median :0.000 ## Mean :0.1963 Mean :0.486 Mean :0.1963 Mean :0.243 ## 3rd Qu.:0.0000 3rd Qu.:1.000 3rd Qu.:0.0000 3rd Qu.:0.000 ## Max. :1.0000 Max. :1.000 Max. :1.0000 Max. :1.000 ## R9 D12 R10 R11 ## Min. :0.000 Min. :0.0000 Min. :0.0000 Min. :0.0000 ## 1st Qu.:0.000 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :1.000 Median :0.0000 Median :0.0000 Median :0.0000 ## Mean :0.514 Mean :0.2617 Mean :0.1028 Mean :0.4206 ## 3rd Qu.:1.000 3rd Qu.:1.0000 3rd Qu.:0.0000 3rd Qu.:1.0000 ## Max. :1.000 Max. :1.0000 Max. :1.0000 Max. :1.0000 ## H11 R12 D13 R13 ## Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :1.000 ## 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:1.000 ## Median :0.0000 Median :1.0000 Median :0.0000 Median :1.000 ## Mean :0.1589 Mean :0.7103 Mean :0.1028 Mean :1.533 ## 3rd Qu.:0.0000 3rd Qu.:1.0000 3rd Qu.:0.0000 3rd Qu.:2.000 ## Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :4.000 ## R14 H12 R15 H13 ## Min. :1.000 Min. :1.000 Min. :1.000 Min. :1.000 ## 1st Qu.:2.000 1st Qu.:1.500 1st Qu.:2.000 1st Qu.:1.000 ## Median :3.000 Median :2.000 Median :2.000 Median :2.000 ## Mean :2.589 Mean :1.935 Mean :2.121 Mean :2.084 ## 3rd Qu.:3.000 3rd Qu.:2.000 3rd Qu.:3.000 3rd Qu.:3.000 ## Max. :4.000 Max. :4.000 Max. :4.000 Max. :4.000 ## R16 R17 R18 D14 ## Min. :2.000 Min. :1.000 Min. :1.000 Min. :1.000 ## 1st Qu.:3.000 1st Qu.:2.000 1st Qu.:1.000 1st Qu.:1.000 ## Median :3.000 Median :2.000 Median :2.000 Median :2.000 ## Mean :3.252 Mean :2.439 Mean :1.869 Mean :1.692 ## 3rd Qu.:4.000 3rd Qu.:3.000 3rd Qu.:2.000 3rd Qu.:2.000 ## Max. :4.000 Max. :4.000 Max. :4.000 Max. :3.000 ## R19 H14 R20 D15 ## Min. :1.000 Min. :1.000 Min. :1.000 Min. :1.000 ## 1st Qu.:1.000 1st Qu.:2.000 1st Qu.:1.000 1st Qu.:1.000 ## Median :1.000 Median :3.000 Median :2.000 Median :2.000 ## Mean :1.542 Mean :2.579 Mean :1.841 Mean :1.991 ## 3rd Qu.:2.000 3rd Qu.:4.000 3rd Qu.:2.000 3rd Qu.:3.000 ## Max. :4.000 Max. :4.000 Max. :4.000 Max. :4.000 ## R21 D16 ## Min. :0.000 Min. :1.000 ## 1st Qu.:1.000 1st Qu.:1.000 ## Median :2.000 Median :2.000 ## Mean :2.215 Mean :1.785 ## 3rd Qu.:3.000 3rd Qu.:2.000 ## Max. :5.000 Max. :4.000 The data has now the proper header, however each variable is still coded numerically. 6.2 Data Inspection 6.2.1 Data Type In R, the variables can be of different types, going from numerical to nominal to binary etc. This section aims in presenting the most common types (and their properties) used in sensory and consumer studies, and in showing how to transform a variable from one type to another. Remember that when your dataset is a tibble (as is the case here), the type of each variable is provided as sub-header when printed on screen. This eases the work of the analyst as the variables type can be assessed at any moment. In case the dataset is not in a tibble, the use of the str() function used previously becomes handy as it provides this information. In sensory and consumer research, the four most common types are: Numerical (incl. integer or int, decimal or dcl, and double or dbl); Logical or lgl; Character or char; Factor or fct. R still has plenty of other types, for more information please visit: https://tibble.tidyverse.org/articles/types.html 6.2.1.1 Numerical Data Since a large proportion of the research done is quantitative, it is no surprise that our dataset are often dominated with numerical variables. In practice, numerical data includes integer (non-fractional number, e.g. 1, 2, -16, etc.), or decimal value (or double, e.g. 1.6, 2.333333, -3.2 etc.). By default, when reading data from an external file, R converts any numerical variables to integer unless decimal points are detected, in which case it is converted into double. Do we want to show how to format R wrt the number of decimals? (e.g. options(digits=2)) 6.2.1.2 Binary Data Another common type that seem to be numerical in appearance, but that has additional properties is the binary type. Binary data is data that takes two possible values (TRUE or FALSE), and are often the results of a test (e.g. is x&gt;3? Or is MyVar numerical?). A typical example of binary data in sensory and consumer research is data collected through Check-All-That-Apply (CATA) questionnaires. Note: Intrinsically, binary data is numerical, TRUE being assimilated to 1, FALSE to 0. If multiple tests are being performed, it is possible to sum the number of tests that pass using the sum() function, as shown in the simple example below: set.seed(123456) # Generating 10 random values between 1 and 10 using the uniform distribution x &lt;- runif(10, 1, 10) x ## [1] 8.180059 7.782086 4.521301 4.074010 4.251647 2.785103 5.813722 1.868736 ## [9] 9.890622 2.508125 # Test whether the values generated are strictly larger than 5 test &lt;- x&gt;5 test ## [1] TRUE TRUE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE # Counting the number of values strictly larger than 5 sum(test) ## [1] 4 6.2.1.3 Nominal Data Nominal data is any data that is not numerical. In most cases, nominal data are defined through text, or strings. It can appear in some situations that nominal variables are still defined with numbers although they do not have a numerical meaning. This is for instance the case when the respondents or samples are identified through numerical codes: In that case, it is clear that respondent 2 is not twice larger than respondent 1 for instance. But since the software cannot guess that those numbers are identifiers rather than numbers, the variables should be declared as nominal. The procedure explaining how to convert the type of the variables will be explained in the next section. For nominal data, two particular types of data are of interest: Character or char; Factor or fct. Variables defined as character or factor take strings as input. However, these two types differ in terms of structure of their levels: For character, there are no particular structure, and the variables can take any values (e.g. open-ended question); For factor, the inputs of the variables are structured into levels. To evaluate the number of levels, different procedure are required: For character, one should count the number of unique element using length() and unique(); For factor, the levels and the number of levels are direcly provided by levels() and nlevels(). Lets compare a variable set as factor and character by using the Judge column from TFEQ_data: example &lt;- TFEQ_data %&gt;% dplyr::select(Judge) %&gt;% mutate(Judge_fct = as.factor(Judge)) print(&quot;Summary:&quot;) ## [1] &quot;Summary:&quot; summary(example) ## Judge Judge_fct ## Length:107 J1 : 1 ## Class :character J10 : 1 ## Mode :character J100 : 1 ## J101 : 1 ## J103 : 1 ## J105 : 1 ## (Other):101 print(&quot;As Character:&quot;) ## [1] &quot;As Character:&quot; unique(example$Judge) ## [1] &quot;J48&quot; &quot;J61&quot; &quot;J60&quot; &quot;J97&quot; &quot;J38&quot; &quot;J26&quot; &quot;J103&quot; &quot;J91&quot; &quot;J13&quot; &quot;J73&quot; ## [11] &quot;J49&quot; &quot;J62&quot; &quot;J14&quot; &quot;J98&quot; &quot;J15&quot; &quot;J39&quot; &quot;J74&quot; &quot;J64&quot; &quot;J99&quot; &quot;J75&quot; ## [21] &quot;J108&quot; &quot;J76&quot; &quot;J1&quot; &quot;J65&quot; &quot;J63&quot; &quot;J2&quot; &quot;J24&quot; &quot;J27&quot; &quot;J3&quot; &quot;J50&quot; ## [31] &quot;J4&quot; &quot;J77&quot; &quot;J66&quot; &quot;J5&quot; &quot;J67&quot; &quot;J6&quot; &quot;J100&quot; &quot;J90&quot; &quot;J92&quot; &quot;J7&quot; ## [41] &quot;J79&quot; &quot;J68&quot; &quot;J69&quot; &quot;J85&quot; &quot;J101&quot; &quot;J70&quot; &quot;J51&quot; &quot;J52&quot; &quot;J109&quot; &quot;J8&quot; ## [51] &quot;J93&quot; &quot;J53&quot; &quot;J54&quot; &quot;J110&quot; &quot;J94&quot; &quot;J111&quot; &quot;J86&quot; &quot;J16&quot; &quot;J112&quot; &quot;J29&quot; ## [61] &quot;J95&quot; &quot;J96&quot; &quot;J118&quot; &quot;J17&quot; &quot;J117&quot; &quot;J55&quot; &quot;J30&quot; &quot;J40&quot; &quot;J41&quot; &quot;J9&quot; ## [71] &quot;J31&quot; &quot;J10&quot; &quot;J56&quot; &quot;J87&quot; &quot;J71&quot; &quot;J42&quot; &quot;J43&quot; &quot;J32&quot; &quot;J81&quot; &quot;J58&quot; ## [81] &quot;J19&quot; &quot;J33&quot; &quot;J34&quot; &quot;J44&quot; &quot;J72&quot; &quot;J113&quot; &quot;J45&quot; &quot;J105&quot; &quot;J114&quot; &quot;J46&quot; ## [91] &quot;J20&quot; &quot;J82&quot; &quot;J115&quot; &quot;J59&quot; &quot;J116&quot; &quot;J21&quot; &quot;J88&quot; &quot;J83&quot; &quot;J22&quot; &quot;J11&quot; ## [101] &quot;J35&quot; &quot;J89&quot; &quot;J120&quot; &quot;J12&quot; &quot;J36&quot; &quot;J23&quot; &quot;J119&quot; length(unique(example$Judge)) ## [1] 107 print(&quot;As Factor:&quot;) ## [1] &quot;As Factor:&quot; levels(example$Judge_fct) ## [1] &quot;J1&quot; &quot;J10&quot; &quot;J100&quot; &quot;J101&quot; &quot;J103&quot; &quot;J105&quot; &quot;J108&quot; &quot;J109&quot; &quot;J11&quot; &quot;J110&quot; ## [11] &quot;J111&quot; &quot;J112&quot; &quot;J113&quot; &quot;J114&quot; &quot;J115&quot; &quot;J116&quot; &quot;J117&quot; &quot;J118&quot; &quot;J119&quot; &quot;J12&quot; ## [21] &quot;J120&quot; &quot;J13&quot; &quot;J14&quot; &quot;J15&quot; &quot;J16&quot; &quot;J17&quot; &quot;J19&quot; &quot;J2&quot; &quot;J20&quot; &quot;J21&quot; ## [31] &quot;J22&quot; &quot;J23&quot; &quot;J24&quot; &quot;J26&quot; &quot;J27&quot; &quot;J29&quot; &quot;J3&quot; &quot;J30&quot; &quot;J31&quot; &quot;J32&quot; ## [41] &quot;J33&quot; &quot;J34&quot; &quot;J35&quot; &quot;J36&quot; &quot;J38&quot; &quot;J39&quot; &quot;J4&quot; &quot;J40&quot; &quot;J41&quot; &quot;J42&quot; ## [51] &quot;J43&quot; &quot;J44&quot; &quot;J45&quot; &quot;J46&quot; &quot;J48&quot; &quot;J49&quot; &quot;J5&quot; &quot;J50&quot; &quot;J51&quot; &quot;J52&quot; ## [61] &quot;J53&quot; &quot;J54&quot; &quot;J55&quot; &quot;J56&quot; &quot;J58&quot; &quot;J59&quot; &quot;J6&quot; &quot;J60&quot; &quot;J61&quot; &quot;J62&quot; ## [71] &quot;J63&quot; &quot;J64&quot; &quot;J65&quot; &quot;J66&quot; &quot;J67&quot; &quot;J68&quot; &quot;J69&quot; &quot;J7&quot; &quot;J70&quot; &quot;J71&quot; ## [81] &quot;J72&quot; &quot;J73&quot; &quot;J74&quot; &quot;J75&quot; &quot;J76&quot; &quot;J77&quot; &quot;J79&quot; &quot;J8&quot; &quot;J81&quot; &quot;J82&quot; ## [91] &quot;J83&quot; &quot;J85&quot; &quot;J86&quot; &quot;J87&quot; &quot;J88&quot; &quot;J89&quot; &quot;J9&quot; &quot;J90&quot; &quot;J91&quot; &quot;J92&quot; ## [101] &quot;J93&quot; &quot;J94&quot; &quot;J95&quot; &quot;J96&quot; &quot;J97&quot; &quot;J98&quot; &quot;J99&quot; nlevels(example$Judge_fct) ## [1] 107 Although Judge and Judge_fct look the same, they are structurally different, and those differences play an important role that one should consider when running certain analyses, or building tables and graphs. When set as character, the number of levels of a variable is directly read from the data, and its levels order would either match the way they appear in the data, or are ordered alphabetically. This means that any data collected using a structured scale will lose its natural order. When set as factor, the number and order of the factor levels are informed, and does not depend on the data itself: If a level has never been selected, or if certain groups have been filtered, this information is still present in the data. To illustrate this, lets re-arrange the levels from Judge_fct by ordering them numerically in such a way J2 follows J1 rather than J10. judge &lt;- str_sort(levels(example$Judge_fct),numeric=TRUE) judge ## [1] &quot;J1&quot; &quot;J2&quot; &quot;J3&quot; &quot;J4&quot; &quot;J5&quot; &quot;J6&quot; &quot;J7&quot; &quot;J8&quot; &quot;J9&quot; &quot;J10&quot; ## [11] &quot;J11&quot; &quot;J12&quot; &quot;J13&quot; &quot;J14&quot; &quot;J15&quot; &quot;J16&quot; &quot;J17&quot; &quot;J19&quot; &quot;J20&quot; &quot;J21&quot; ## [21] &quot;J22&quot; &quot;J23&quot; &quot;J24&quot; &quot;J26&quot; &quot;J27&quot; &quot;J29&quot; &quot;J30&quot; &quot;J31&quot; &quot;J32&quot; &quot;J33&quot; ## [31] &quot;J34&quot; &quot;J35&quot; &quot;J36&quot; &quot;J38&quot; &quot;J39&quot; &quot;J40&quot; &quot;J41&quot; &quot;J42&quot; &quot;J43&quot; &quot;J44&quot; ## [41] &quot;J45&quot; &quot;J46&quot; &quot;J48&quot; &quot;J49&quot; &quot;J50&quot; &quot;J51&quot; &quot;J52&quot; &quot;J53&quot; &quot;J54&quot; &quot;J55&quot; ## [51] &quot;J56&quot; &quot;J58&quot; &quot;J59&quot; &quot;J60&quot; &quot;J61&quot; &quot;J62&quot; &quot;J63&quot; &quot;J64&quot; &quot;J65&quot; &quot;J66&quot; ## [61] &quot;J67&quot; &quot;J68&quot; &quot;J69&quot; &quot;J70&quot; &quot;J71&quot; &quot;J72&quot; &quot;J73&quot; &quot;J74&quot; &quot;J75&quot; &quot;J76&quot; ## [71] &quot;J77&quot; &quot;J79&quot; &quot;J81&quot; &quot;J82&quot; &quot;J83&quot; &quot;J85&quot; &quot;J86&quot; &quot;J87&quot; &quot;J88&quot; &quot;J89&quot; ## [81] &quot;J90&quot; &quot;J91&quot; &quot;J92&quot; &quot;J93&quot; &quot;J94&quot; &quot;J95&quot; &quot;J96&quot; &quot;J97&quot; &quot;J98&quot; &quot;J99&quot; ## [91] &quot;J100&quot; &quot;J101&quot; &quot;J103&quot; &quot;J105&quot; &quot;J108&quot; &quot;J109&quot; &quot;J110&quot; &quot;J111&quot; &quot;J112&quot; &quot;J113&quot; ## [101] &quot;J114&quot; &quot;J115&quot; &quot;J116&quot; &quot;J117&quot; &quot;J118&quot; &quot;J119&quot; &quot;J120&quot; levels(example$Judge_fct) &lt;- judge Now the levels are sorted, lets remove some respondents by only keeping the 20 first ones (J1 to J20, as J18 does not exist), and re-run the previous code: example &lt;- TFEQ_data %&gt;% dplyr::select(Judge) %&gt;% mutate(Judge_fct = as.factor(Judge)) %&gt;% filter(Judge %in% paste0(&quot;J&quot;,1:20)) dim(example) ## [1] 19 2 print(&quot;As Character:&quot;) ## [1] &quot;As Character:&quot; unique(example$Judge) ## [1] &quot;J13&quot; &quot;J14&quot; &quot;J15&quot; &quot;J1&quot; &quot;J2&quot; &quot;J3&quot; &quot;J4&quot; &quot;J5&quot; &quot;J6&quot; &quot;J7&quot; &quot;J8&quot; &quot;J16&quot; ## [13] &quot;J17&quot; &quot;J9&quot; &quot;J10&quot; &quot;J19&quot; &quot;J20&quot; &quot;J11&quot; &quot;J12&quot; length(unique(example$Judge)) ## [1] 19 print(&quot;As Factor:&quot;) ## [1] &quot;As Factor:&quot; levels(example$Judge_fct) ## [1] &quot;J1&quot; &quot;J10&quot; &quot;J100&quot; &quot;J101&quot; &quot;J103&quot; &quot;J105&quot; &quot;J108&quot; &quot;J109&quot; &quot;J11&quot; &quot;J110&quot; ## [11] &quot;J111&quot; &quot;J112&quot; &quot;J113&quot; &quot;J114&quot; &quot;J115&quot; &quot;J116&quot; &quot;J117&quot; &quot;J118&quot; &quot;J119&quot; &quot;J12&quot; ## [21] &quot;J120&quot; &quot;J13&quot; &quot;J14&quot; &quot;J15&quot; &quot;J16&quot; &quot;J17&quot; &quot;J19&quot; &quot;J2&quot; &quot;J20&quot; &quot;J21&quot; ## [31] &quot;J22&quot; &quot;J23&quot; &quot;J24&quot; &quot;J26&quot; &quot;J27&quot; &quot;J29&quot; &quot;J3&quot; &quot;J30&quot; &quot;J31&quot; &quot;J32&quot; ## [41] &quot;J33&quot; &quot;J34&quot; &quot;J35&quot; &quot;J36&quot; &quot;J38&quot; &quot;J39&quot; &quot;J4&quot; &quot;J40&quot; &quot;J41&quot; &quot;J42&quot; ## [51] &quot;J43&quot; &quot;J44&quot; &quot;J45&quot; &quot;J46&quot; &quot;J48&quot; &quot;J49&quot; &quot;J5&quot; &quot;J50&quot; &quot;J51&quot; &quot;J52&quot; ## [61] &quot;J53&quot; &quot;J54&quot; &quot;J55&quot; &quot;J56&quot; &quot;J58&quot; &quot;J59&quot; &quot;J6&quot; &quot;J60&quot; &quot;J61&quot; &quot;J62&quot; ## [71] &quot;J63&quot; &quot;J64&quot; &quot;J65&quot; &quot;J66&quot; &quot;J67&quot; &quot;J68&quot; &quot;J69&quot; &quot;J7&quot; &quot;J70&quot; &quot;J71&quot; ## [81] &quot;J72&quot; &quot;J73&quot; &quot;J74&quot; &quot;J75&quot; &quot;J76&quot; &quot;J77&quot; &quot;J79&quot; &quot;J8&quot; &quot;J81&quot; &quot;J82&quot; ## [91] &quot;J83&quot; &quot;J85&quot; &quot;J86&quot; &quot;J87&quot; &quot;J88&quot; &quot;J89&quot; &quot;J9&quot; &quot;J90&quot; &quot;J91&quot; &quot;J92&quot; ## [101] &quot;J93&quot; &quot;J94&quot; &quot;J95&quot; &quot;J96&quot; &quot;J97&quot; &quot;J98&quot; &quot;J99&quot; nlevels(example$Judge_fct) ## [1] 107 After filtering some respondents, it can be noticed that the variable set as character only contains 19 elements, whereas the column set as factor still contains the 107 respondents (most of them not having any recordings). This property can be seen as an advantage or a disadvantage depending on the situation: For frequencies, it may be relevant to remember all the options, including the ones that may never be selected, and to order the results logically (use of factor). For hypothesis testing (e.g. ANOVA) on subset of data (e.g. the data being split by gender), the Judge variable set as character would have the correct number of degrees of freedom (18 in our example) whereas the variable set as factor would use 106 degrees of freedom in all cases! The latter point is particularly critical since the analysis is incorrect and will either return an error or worse return erroneous results! Last but not least, variables defined as factor allow having their levels being renamed (and eventually combined) very easily. Lets consider the Living area variable from TFEQ_data as example. From the original excel file, it can be seen that it has three levels, 1 corresponding to urban area, 2 to rurban area, and 3 to rural area. Lets start by renaming this variable accordingly: example = TFEQ_data %&gt;% mutate(Area = factor(`Living area`, levels=c(1,2,3), labels=c(&quot;Urban&quot;, &quot;Rurban&quot;, &quot;Rural&quot;))) levels(example$Area) ## [1] &quot;Urban&quot; &quot;Rurban&quot; &quot;Rural&quot; nlevels(example$Area) ## [1] 3 table(example$`Living area`, example$Area) ## ## Urban Rurban Rural ## 1 72 0 0 ## 2 0 12 0 ## 3 0 0 23 As can be seen, the variable Area is the factor version (including its labels) of Living area. If we would also consider that Rurban should be combined with Rural, and that Rural should appear before Urban, we can simply modify the code as such: example = TFEQ_data %&gt;% mutate(Area = factor(`Living area`, levels=c(2,3,1), labels=c(&quot;Rural&quot;, &quot;Rural&quot;, &quot;Urban&quot;))) levels(example$Area) ## [1] &quot;Rural&quot; &quot;Urban&quot; nlevels(example$Area) ## [1] 2 table(example$`Living area`, example$Area) ## ## Rural Urban ## 1 0 72 ## 2 12 0 ## 3 23 0 This approach of renaming and re-ordering factor levels is very important as it simplifies the readability of tables and figures. Some other transformations can be applied to factors thanks to the {forcats} package. Amongst other relevant functions, particular attention can be given to: fct_reorder/fct_reorder2 and fct_relevel which reorder the levels of a factor; fct_recode which helps recoding a factor (as an alternative to factor used in the previous example); fct_collapse and fct_lump which allows aggregating different levels together (fct_lump regroups automatically all the rare levels). Although it hasnt been done here, manipulating strings is also possible. To do so, the {stringr} package provides a lot of of interesting functions, such as: str_to_upper/str_to_lower which automatically convert strings to uppercase or lowercase; str_c, str_sub which combine or subset strings; str_trim and str_squish which help remove white spaces; str_extract, str_replace, str_split to extract, replace, or split strings or part of the strings. 6.2.2 Changing the type of a variable Transforming the type of variables using mutate(): from logical to numerical; from numerical to character/factor; from character/factor to numerical 6.3 Data Organization Presentation of the different shapes of the tables based on objectives 6.4 Data Manipulation 6.4.1 Type of table matrix, data frame, and tibble. how to check the type? class() how to test it? is.data.frame(), is.matrix(), is_tibble() how to convert it to another format? (see below) Note on {FactoMineR} and {SensoMineR} which require data frames or matrix (not tibble) so introduction to column_to_rownames() and rownames_to_columns() as well as as.data.frame() and as_tibble(). 6.4.2 Data organisation select(), filter(), arrange() mutate() and transmute() 6.4.3 Data re-structuration pivot_wider() and pivot_longer() full_join(), inner_join(), left_join() and right_join() unnest_token() from {tidytext} 6.5 Cleaning and Quality Assessment 6.5.1 Renaming renaming columns using rename() or select() renaming elements using factor() and {forcats} 6.5.2 Recoding Example of recoding (could be renaming, or replacing NAs, etc.) by combining mutate() and ifelse() 6.5.3 Handling Missing Values Removing and replacing NAs 6.5.4 Quality Assessment Graphics? "],["data-analysis.html", "Chapter 7 Data Analysis 7.1 Transformation 7.2 Exploration 7.3 Modeling", " Chapter 7 Data Analysis 7.1 Transformation 7.2 Exploration 7.3 Modeling "],["data-viz.html", "Chapter 8 Data Visualization 8.1 Principles 8.2 Table Mechanics 8.3 Chart Mechanics 8.4 Examples 8.5 Raw Material 8.6 Philosophy of ggplot2 8.7 Present the most useful graph 8.8 Interesting addition", " Chapter 8 Data Visualization 8.1 Principles 8.2 Table Mechanics 8.3 Chart Mechanics 8.4 Examples 8.5 Raw Material 8.6 Philosophy of ggplot2 Explain the principles of multi-layer graphs through an example. aes(), geom_(), theme() 8.6.1 aesthetics Provide the most relevant options for aes() x, y, z group color, fill text, label alpha, size 8.6.2 geom_ Explain the fact that some geom_() comes with some stats automatically (e.g. geom_bar bins the data) 8.7 Present the most useful graph 8.7.1 Scatter points geom_point() 8.7.2 Line charts geom_line(), geom_smooth() geom_abline() geom_hline() and geom_vline() geom_segment() and geom_arrow() 8.7.3 Bar charts geom_bar, geom_polygon, geom_histogram(), geom_freqpoly() position=\"identity\", position=\"dodge\" or position=\"fill\" 8.7.4 Distribution geom_boxplot() and geom_violin() 8.7.5 Text geom_text and geom_label presentation of {ggrepel} 8.7.6 Rectangles geom_tile(), geom_rect, and geom_raster() 8.7.7 Themes and legend theme(), and pre-defined themes like theme_bw(), theme_minimal(), etc. ggtitle() xlab(), ylab(), or labs() 8.8 Interesting addition 8.8.1 Playing around with axes coord_fixed(), coord_cartesian(), coord_trans() scale_x_, scale_y_ 8.8.2 Transposing the plot coord_flip() and coord_polar() 8.8.3 Splitting plots facet_wrap(), facet_grid() 8.8.4 Combining plots {patchwork} "],["value-communication.html", "Chapter 9 Value Communication 9.1 Design principles 9.2 Scientific inquiry vs storytelling 9.3 Research reformulation 9.4 Interactive reporting 9.5 Excel 9.6 Word 9.7 PowerPoint 9.8 HTML 9.9 Raw Material 9.10 MORE RAW MATERIAL 9.11 PowerPint Slide Master 9.12 Placeholders 9.13 Text 9.14 Tables 9.15 Charts 9.16 Word", " Chapter 9 Value Communication 9.1 Design principles 9.2 Scientific inquiry vs storytelling 9.3 Research reformulation 9.4 Interactive reporting 9.5 Excel 9.6 Word 9.7 PowerPoint 9.7.1 Charts 9.7.2 Tables 9.7.3 Bullet Points 9.7.4 Images 9.8 HTML 9.9 Raw Material TUTORIAL TITLE Increasing Business Impact through Automated Reporting in R TUTORIAL DESCRIPTION With this tutorial, we help sensory scientists increase their business impact by leveraging tools for automated report production in R. ABSTRACT Effective communication of results is among the essential duties of the sensory scientist, but the sometimes tedious mechanics of report production together with the sheer volume of data that many scientists now must process combine to make reporting design an afterthought in too many cases. In this tutorial, we review recent advances in automated report production that liberate resources for scientists to focus on the interpretation and communication of results, while simultaneously reducing errors and increasing the consistency of their analyses. We teach the tutorial through an extended example, cumulatively building an R script that takes participates from receipt of an example dataset to a beautifully-designed and nearly completed PowerPoint presentation automatically and using freely available, open-source packages. Details of how to customize the final presentation to incorporate corporate branding - such as logos, font choices, and color palettes - will also be covered. TOPICS COVERED INCLUDE - What does automated reporting mean in practice? - Scripting analyses, tables, and charts - Automated production of PowerPoint presentations - Building a cookbook of reporting recipes - Font choices and color palettes - Layering storytelling onto an automated report SOFTWARE PACKAGES We use the R programming environment along with several freely available packages such as those that comprise the tidyverse, extrafont, officer, openxlsx, and RColorBrewer. Before the conference, we will provide a full list of packages and versions required to run the script created during the tutorial. Outline of Section What is Automated Reporting? [Pull from presentation] Why Script? Save time Reduce errors Collaboration Share code with others Read own code later Explain choices for analysis, table presentation, charts Save steps for result creation Main tools R/RStudio RMarkdown, HTML output, etc. (mention but dont focus) Packages for Microsoft office production Officer suite (PowerPoint, Word) Charts, especially RVG Extract from Word/PowerPoint Index Flextable Images? Packages for formatting extrafont extrafontdb Rcolorbrewer 9.9.1 PowerPoint workflow Start with template Explain slide master How to adjust choices Internal naming (relevant later) Example Title slides Tables Charts Bullet points Images Layout discussion 9.9.2 Word Touch on but not in detail Shiny Make dashboards to help non-users (or self) 9.9.3 Excel Although Excel is not our preferred tool for automated reporting, it is still one of the major ways to access and share data. Most data collection software offers the possibility to export data and/or results in an Excel format, and most data analysis tools accepts Excel format as inputs. With the large use of Excel, it is no surprise that many of our colleagues or clients like to share data and results using such spreadsheets. It is even less a surprise that R provides multiple solutions to import/export results from/to Excel. For the importation of datasets, we have already presented two packages ({xlsx} and {readxl}) which provide some interesting options: they will not be detailed any further here. For exporting results, two complementary packages (amongst others) in terms of ease of use and flexibility in the outcome are considered: {xlsx} and {openxlsx}. Besides its option of importing directly Excel files (.xls and .xlsx), the package {xlsx} also offers the option to export tables in Excel through the function write.xlsx(). write.xlsx() works in the same way as base::write.csv() (as its name indicates, this function exports tables from R to a .csv file), but since it generates an Excel file (.xls or .xlsx), additional tabs can be added using the append option. The procedure for write.xlsx() is the following: First export the table x=mydata1 of interest in a new Excel into the tab entitles mytab1 of the file myfile.xlsx using the options file=myfile.xlsx and sheetName= mytab1, and set the option append=FALSE. Then add to file=myfile.xlsx any additional table x=mydata2 in another tab (e.g. sheet=mytab2) using a similar command, but by setting append=TRUE. By doing so, R understands that myfile.xlsx should be extended with a second tab called mytab2 that contains mytable2. The tables exported may or may not include row names and column namess, and the sheet can automatically be password protected using the password option. library(xlsx) # We propose here to export the 5 first tables in 5 different tabs. for (v in 1:5){ if (v==1){ append=FALSE } else { append=TRUE } xlsx::write.xlsx(as.data.frame(demog_freq[[v]]), file=&quot;thierry_code/Output/Tables using xlsx.xlsx&quot;, sheetName=names(demog_freq)[v], row.names=FALSE, append=append) } The exportation of tables using the {xlsx} package is easy, yet very simplistic as it does not allow formatting the tables, nor does it allow exporting multiple tables in the same tab. For more advanced exporting options, the use of {openxlsx} package is preferred as it allows more flexibility in structuring and formatting the Excel output. With {openxlsx}, the procedure starts with creating a workbook wb using the createWorkbook() function, to which we add worksheets through the addWorksheet() function. On a given worksheet, any table can be exported using writeData() or writeDataTable(), which controls where to write the table through the startRow and startCol options. Through these different functions, many additional formatting procedure can be applied: createWorksheet() allows: show/hide grid lines using gridLines; colour the specific sheet using tabColour; change the zoom on the sheet through zoom; show/hide the tab using visible; format the worksheet by specifying its size (paperSize) and orientation (orientation). writeData() and writeDataTable() allow: controlling where to print the data using startRow and startCol (or alternatively xy: xy=c(B,12) prints the table in cell B12), hence allowing exporting multiple tables within the same tab; including the row names and column names through rowNames and colNames; formatting the header using headerStyle (incl. colour of the text and/or background, font, font size, etc.) and whether a filter should be applied; shaping the borders using predefined solutions through borders, or customizing them with borderStyle and borderColour; adding a filter to the table using withFilger; converts missing data to #N/A or any other string using keepNA and na.string. Additional formatting can be controlled using: options() to predefine number formatting, border colours and style that will be applied automatically to each tables; modifyBaseFont() to defined the font and font size; freezePane() to freeze the first row and/or column of the table using firstRow=TRUE and firstCol=TRUE; createStyle() to pre-define a style, or addStyle() to apply a later stage some styling to some cells; controls the width of the columns using setColWidths; conditionalFormatting() allows coloring cells when they meet pre-defined rules, as for instance to highlight significant p-values. When using {openxlsx}, we recommend to use the same procedure as for Word and PowerPoint: Start with setting as default the parameters that should be applied to each table; Create styles for text or table headers that you save in different elements, and that you apply where needed. In the following example, the sensory profiles are exported in the first tab, and a set of frequency tables are exported in the second tab. To introduce conditional formatting with {openxlsx}, the sensory profiles are color coded as following: For each cell, the value is compared to the overall mean computed for that column and is colored in red (resp. blue) if its higher (resp. lower) than the mean. In practice, the color style is pre-defined in two parameters called pos_style (red) and neg_style (blue) using createStyle(). The decision whether pos_style or neg_style should be used is defined by the rule parameter from the conditionalFormatting()1 function. library(openxlsx) # Pre-define options to control the borders options(&quot;openxlsx.borderColour&quot;=&quot;#4F80BD&quot;) options(&quot;openxlsx.borderStyle&quot;=&quot;thin&quot;) # Automatically set Number formats to 3 values after the decimal options(&quot;openxlsx.numFmt&quot;=&quot;0.000&quot;) # Change the font to Calibri size 10 modifyBaseFont(wb, fontSize=10, fontName=&quot;Calibri&quot;) # Create a header style in which # the text is centered and in bold, # borders on top and on the bottom, # a blue background is used. headSty &lt;- createStyle(fgFill=&quot;#DCE6F1&quot;, border=&quot;TopBottom&quot;, halign=&quot;center&quot;, textDecoration=&quot;bold&quot;) # Preparing the colouring for the conditional formatting senso_mean &lt;- sensory %&gt;% group_by(product) %&gt;% summarise_if(is.numeric, mean) %&gt;% column_to_rownames(var=&quot;product&quot;) overall_mean &lt;- apply(senso_mean, 2, mean) pos_style &lt;- createStyle(fontColour=&quot;firebrick3&quot;, bgFill=&quot;mistyrose1&quot;) neg_style &lt;- createStyle(fontColour=&quot;navy&quot;, bgFill=&quot;lightsteelblue&quot;) # Creation of the workbook wb with two tabs called Mean and Frequency wb &lt;- openxlsx::createWorkbook() addWorksheet(wb, sheetName=&quot;Mean&quot;, gridLines=FALSE) addWorksheet(wb, sheetName=&quot;Frequency&quot;, gridLines=FALSE) # Exporting senso_mean to the first tab (first row and first column are frozen) # A pre-defined Excel design called TableStyleLight9 is used for this table freezePane(wb, sheet=1, firstRow=TRUE, firstCol=TRUE) writeDataTable(wb, sheet=1, x=senso_mean, colNames=TRUE, rowNames=TRUE, tableStyle=&quot;TableStyleLight9&quot;) for (v in 1:ncol(senso_mean1)){ conditionalFormatting(wb, 1, cols=v+1, rows=(1:nrow(senso_mean1)+1), rule = paste0(&quot;&gt;&quot;,overall_mean[[v]]), style = pos_style) conditionalFormatting(wb, 1, cols=v+1, rows=(1:nrow(senso_mean1)+1), rule = paste0(&quot;&lt;&quot;,overall_mean[[v]]), style = neg_style) } # Here, we export many different frequency tables in the second tab by: # 1. adding a title prior to the table # 2. export the table under its title # 3. title_r and next_r are used to track where the text is being printed writeData(wb, 2, x=&quot;Frequency Tables&quot;, startRow=1, startCol=2) title_r &lt;- 3 for (v in 1:5){ writeData(wb, 2, x=names(demog_freq)[v], startRow=max(title_r), startCol=1) writeData(wb, 2, x=demog_freq[[v]], startRow=max(title_r)+1, startCol=1, rowNames=FALSE, borders=&quot;surrounding&quot;, headerStyle=headSty) next_r &lt;- max(title_r) + 1 + 1 + nrow(demog_freq[[v]]) + 1 title_r &lt;- c(title_r, next_r) } # We apply different style for the main title and for each table&#39;s title. addStyle(wb, 2, style=createStyle(fontSize=14, textDecoration=&quot;bold&quot;), rows=1, cols=2) addStyle(wb, 2, style=createStyle(fontSize=12, textDecoration=&quot;italic&quot;), rows=title_r, cols=rep(1,length(title_r))) setColWidths(wb, 2, cols=1:10, widths=12) The file is created using saveWorkbook() by specifying the name of the workbook wb and its path through file. In case such workbook already exists, it can be overwritten using overwrite. Additionally, the user can visualize the file so far created using openXL(). For more details on using {openxlsx} see https://rdrr.io/cran/openxlsx/. 9.10 MORE RAW MATERIAL library(tidyverse) library(officer) library(flextable) 9.11 PowerPint Slide Master 9.11.1 Importing the Template my_file &lt;- file.path(&quot;input&quot;,&quot;templates&quot;,&quot;Tutorial Template.pptx&quot;) %&gt;% read_pptx() class(my_file) # checking if correct class my_file %&gt;% layout_summary() 9.11.2 Creating a PowerPoint Deck pptx_obj &lt;- read_pptx () # new empty file pptx_obj &lt;- pptx_obj %&gt;% add_slide(layout = &#39;Title and Content&#39;,master = &quot;Office Theme&quot;) pptx_obj %&gt;% print(target = &quot;output/first_example.pptx&quot;) We can not load the themes of Office ex nihilo returns error pptx_obj &lt;- read_pptx() # new empty file pptx_obj &lt;- pptx_obj %&gt;% add_slide(layout = &quot;Title and Content&quot;, master = &quot;Integral&quot;) However, we can save an empty pptx with the desired theme and use it as a template pptx_obj &lt;- read_pptx(file.path(&quot;input&quot;,&quot;templates&quot;,&quot;integral.pptx&quot;)) layout_summary(pptx_obj) We can even load a template with more than one theme pptx_obj &lt;- read_pptx(file.path(&quot;input&quot;,&quot;templates&quot;,&quot;multmasters.pptx&quot;)) layout_summary(pptx_obj) 9.11.3 Selection Pane 9.11.4 Key functions: read_pptx(path), layout_summary(x), layout_properties(x), add_slide(x, layout, master), on_slide(x, index), slide_summary(x, index = NULL) 9.11.5 Example Code pptx_obj &lt;- read_pptx() # new empty file pptx_obj &lt;- pptx_obj %&gt;% # add slide add_slide(layout = &quot;Title and Content&quot;, master = &quot;Office Theme&quot;) layout_summary(pptx_obj) # contains only basic layouts layout_properties(pptx_obj) # additional detail pptx_obj &lt;- pptx_obj %&gt;% on_slide(index = 1) # set active slide slide_summary(pptx_obj) # slide is empty 9.12 Placeholders 9.12.1 Placeholders and Shapes # Example 1 my_data &lt;- c(&quot;My functions are:&quot;,&quot;ph_with&quot;,&quot;ph_location_type&quot;) my_type &lt;- &quot;body&quot; pptx_obj &lt;- read_pptx() # new empty file pptx_obj &lt;- pptx_obj %&gt;% add_slide(layout = &quot;Title and Content&quot;, master = &quot;Office Theme&quot;) pptx_obj &lt;- pptx_obj %&gt;% ph_with(value = my_data,location = ph_location_type(type = my_type)) pptx_obj %&gt;% print(target = &quot;output/test2.1.pptx&quot;) # Example 2: my_data &lt;- head(mtcars)[,1:4] my_type &lt;- &quot;body&quot; pptx_obj &lt;- read_pptx() # new empty file pptx_obj &lt;- pptx_obj %&gt;% add_slide(layout = &quot;Title and Content&quot;, master = &quot;Office Theme&quot;) pptx_obj &lt;- pptx_obj %&gt;% ph_with(value = my_data, location = ph_location_type(type = my_type)) pptx_obj %&gt;% print(target = &quot;output/test2.2.pptx&quot;) # Example 3 # We add a text box item in a custom position # The same can be done for an image, logo, custom objects, etc. my_data &lt;- &quot;My text&quot; my_type &lt;- &quot;body&quot; pptx_obj &lt;- read_pptx() # new empty file pptx_obj &lt;- pptx_obj %&gt;% add_slide(layout = &quot;Title and Content&quot;, master = &quot;Office Theme&quot;) #ph_location is a subfunction which takes as argument #left/top/width/height, units are inches pptx_obj &lt;- pptx_obj %&gt;% ph_with(value = my_data, location = ph_location(left = 2, top = 2, width = 3, height = 1)) pptx_obj %&gt;% print(target = &quot;output/test2.3.pptx&quot;) 9.12.2 Key functions: ph_with() 9.13 Text 9.13.1 Working with Text Each new text item added to a PowerPoint via officer is a paragraph object fpar() (formatted paragraph) creates this object my_data &lt;- fpar(&quot;My text&quot;) my_type &lt;- &quot;body&quot; pptx_obj &lt;- read_pptx() # new empty file pptx_obj &lt;- pptx_obj %&gt;% add_slide(layout = &quot;Title and Content&quot;, master = &quot;Office Theme&quot;) ## Add paragraph pptx_obj &lt;- pptx_obj %&gt;% ph_with(value = my_data, location = ph_location_type(type = my_type)) ## Try to add a second paragraph my_data2 &lt;- fpar(&quot;My other text&quot;) my_type &lt;- &quot;body&quot; pptx_obj &lt;- pptx_obj %&gt;% ph_with(value = my_data2, location = ph_location_type(type = my_type) ) pptx_obj %&gt;% print(target = &quot;output/test3.1.pptx&quot;) ## As we see, this code doesnt produce bullet points as we might hope block_list() allows us to wrap multiple paragraphs together my_data &lt;- fpar(&quot;My text&quot;) blank_line &lt;- fpar(&quot;&quot;) my_data2 &lt;- fpar(&quot;My other text&quot;) my_list &lt;- block_list(my_data, blank_line, my_data2) my_type &lt;- &quot;body&quot; pptx_obj &lt;- read_pptx() # new empty file pptx_obj &lt;- pptx_obj %&gt;% add_slide(layout = &quot;Title and Content&quot;, master = &quot;Office Theme&quot;) %&gt;% ph_with(value = my_list, location = ph_location_type(type = my_type) ) pptx_obj %&gt;% print(target = &quot;output/test3.2.pptx&quot;) Use ftext() (formatted text) to edit the text before pasting into paragraphs. ftext() requires a second argument called prop which contains the formatting properties. my_prop &lt;- fp_text(color = &quot;red&quot;, font.size = 16) my_text &lt;- ftext(&quot;Hello&quot;, prop = my_prop) my_par &lt;- fpar(my_text) ## formatted blank_line &lt;- fpar(&quot;&quot;) my_par2 &lt;- fpar(&quot;My other text&quot;) ## unformatted my_list &lt;- block_list(my_par,blank_line, my_par2) pptx_obj &lt;- read_pptx() # new empty file pptx_obj &lt;- pptx_obj %&gt;% add_slide(layout = &quot;Title and Content&quot;, master = &quot;Office Theme&quot;) %&gt;% ph_with(value = my_list, location = ph_location_type(type = my_type) ) pptx_obj %&gt;% print(target = &quot;output/test3.3.pptx&quot;) 9.13.2 Key functions: fpar(), ftext(), fp_text(), block_list() 9.13.3 Example Code my_list &lt;- block_list( fpar(ftext(&quot;Hello&quot;, prop = fp_text(color = &quot;red&quot;, font.size = 16))) , fpar(ftext(&quot;World&quot;, prop = fp_text(color = &quot;blue&quot;, font.size = 14))) ) # The hierarchy is: # block_list &gt; fpar &gt; ftext &gt; fp_text pptx_obj &lt;- read_pptx() %&gt;% add_slide(layout = &quot;Title and Content&quot;, master = &quot;Office Theme&quot;) %&gt;% ph_with(value = block_list( fpar(ftext(&quot;Hello&quot;, prop = fp_text(color = &quot;red&quot;, font.size = 16))) , fpar(ftext(&quot;World&quot;, prop = fp_text(color = &quot;blue&quot;, font.size = 14)))), ph_location_type(type = &quot;body&quot;) ) %&gt;% print(target = &quot;output/test3.4.pptx&quot;) 9.14 Tables 9.14.1 Basic Code my_data &lt;- head(mtcars) pptx_obj &lt;- read_pptx() %&gt;% add_slide(layout = &quot;Title and Content&quot;, master = &quot;Office Theme&quot;) %&gt;% ph_with(value = head(mtcars), location = ph_location_type(type = &quot;body&quot;)) %&gt;% print(target = &quot;output/test4.1.pptx&quot;) 9.14.2 Introduction to flextable base_table &lt;- tutorial_data[[&quot;Age&quot;]] %&gt;% table() %&gt;% enframe(name = &quot;Age&quot;, value = &quot;Count&quot;) %&gt;% mutate(Count = as.integer(Count)) %&gt;% mutate(Percent = format_percent(Count / sum(Count), num_decimals = 2)) base_table flextable: create attractive tables with predefined formatting. Use of %&gt;% is recommended for readability library(flextable) ft_table &lt;- base_table %&gt;% flextable() ft_table # see preliminary result in Viewer tab of RStudio 9.14.3 Demonstration ft_table &lt;- base_table %&gt;% flextable() %&gt;% autofit() %&gt;% # straightforward, column width # ALIGNMENT # alignment of header: we use part argument align(align = &quot;center&quot;, part = &quot;header&quot;) %&gt;% # alignment of content: we can use part = &quot;body&quot; or specify exact lines align(i = 1:nrow(base_table), j = 1:ncol(base_table), align = &quot;center&quot;) ft_table Set font and characters ft_table = ft_table %&gt;% # FONT AND CHARACTERS # each command is independent, there are no nested functions as in officer bold(i = 1, j = 1:3) %&gt;% # first row, all cols italic(i = 3, j = ~Age+Count+Percent) %&gt;% # first row, all cols, using ~ notation fontsize(i = 2, size = 16) %&gt;% font(fontname = &quot;Calibri&quot;) %&gt;% # since no i or j are input, change is for all data font(fontname = &quot;Roboto&quot;, part = &quot;header&quot;) %&gt;% #different font for header color(i = 3, j = 2, color = &quot;red&quot;) %&gt;% # WIDTH AND HEIGHT # all measurements are in inches width(j = 1, width = 4) %&gt;% # column 1 wider height(i = 8, height = 0.5) %&gt;% # row 8 change # CELL COLORS (background) bg(bg = &quot;#0088FF&quot;, part = &quot;header&quot;) %&gt;% # a custom background for the header bg(i = 7:10, bg = &quot;#C5C5C5&quot;) # a custom background for some cells ft_table Set borders #BORDERS # For borders we need to use nested functions (similar to fpar&gt;ftext&gt;fp_text) #fp_border() is the second level function we will use to specify border&quot;s characteristics # as argument it takes color, style, and width my_border &lt;- fp_border(color = &quot;black&quot;, style = &quot;solid&quot;, width = 2) # We use this second level function inside various main border functions # border_outer() # border_inner() # border_inner_h() # border_inner_v() ft_table &lt;- ft_table %&gt;% border_outer(part = &quot;all&quot;, border = my_border) %&gt;% # using predefined border border_inner(part = &quot;body&quot;, border = fp_border(style = &quot;dashed&quot;)) ft_table 9.14.4 Demonstration Output 9.14.5 Reformat original example ## ft parameters header_background_color = a_green body_background_color = a_cream header_text_col = a_cream body_text_col = a_dark_grey total_text_col = a_red border_solid &lt;- fp_border(color = a_dark_grey, width = 2) border_dashed &lt;- fp_border(color = a_dark_grey, width = 1, style = &quot;dashed&quot;) ft &lt;- summary_table %&gt;% flextable() %&gt;% font(fontname = font_name, part = &quot;all&quot;) %&gt;% fontsize(size = font_size, part = &quot;all&quot;) %&gt;% bold(part = &quot;header&quot;) %&gt;% border_remove() %&gt;% border_outer(border = border_solid) %&gt;% border_inner_v(border = border_solid, part = &quot;header&quot;) %&gt;% border_inner_h(border = border_dashed, part = &quot;header&quot;) %&gt;% border_inner_v(border = border_solid, part = &quot;body&quot;) %&gt;% border_inner_h(border = border_dashed, part = &quot;body&quot;) %&gt;% bg(bg = header_background_color, part = &quot;header&quot;) %&gt;% bg(bg = body_background_color, part = &quot;body&quot;) %&gt;% align(align = &quot;center&quot;, part = &quot;all&quot;) %&gt;% color(color = header_text_col, part = &quot;header&quot;) %&gt;% color(color = body_text_col, part = &quot;body&quot;) %&gt;% color(color = total_text_col, part = &quot;body&quot;, i = nrow(summary_table)) %&gt;% italic(part = &quot;body&quot;, i = nrow(summary_table)) %&gt;% bold(part = &quot;body&quot;, i = nrow(summary_table)) %&gt;% autofit(add_w = 1) ft # Add table to slide pptx_obj &lt;- read_pptx() %&gt;% add_slide(layout = &quot;Title and Content&quot;, master = &quot;Office Theme&quot;) %&gt;% ph_with(value = ft, ph_location(left = 2, top = 2, width = 4 )) %&gt;% print(target = &quot;output/test4.7.pptx&quot;) # positions are fixed. We can find exact positions to center the table 9.14.6 key functions: flextable(), align(), bold(), font(), color(), bg(), height() &amp; width(), border_outer() &amp; border_inner() &amp; border_inner_h() &amp; border_inner_v(), autofit() Additional function to learn: merge(), compose() &amp; as_chunk(), style() fix_border_issues() 9.15 Charts 9.15.1 Adding charts as images # We have preloaded a function to plot the chart. # the function is using ggplot2 as plotting library chart_to_plot &lt;- sample_data_list[[&#39;Sample 1&#39;]] %&gt;% make_jar_chart() # code to create a ggplot2 item, we will skip the contents print(chart_to_plot) # see in Plots Window 9.15.1.1 rvg Example # the output is a ggplot2 object # To add this object as a rvg object on a slide, we will use the ph_with_vg # ph_with_vg replaces the ph_with for a rvg object # ph_with_vg_at allows to input a precise position for a chart, using the top/left we know already # all units are in inches # argument code requires print(chart), argument type is a specific place on slide (&quot;body&quot; or other) ## IMPORTANT: ph_with_vg is is deprecated. #old syntaxis ph_with_vg(code = print(chart_to_plot), type = &quot;body&quot;) pptx_obj &lt;- read_pptx() %&gt;% add_slide(layout = &quot;Title and Content&quot;, master = &quot;Office Theme&quot;) %&gt;% ph_with(value = dml(ggobj = chart_to_plot), location = ph_location_type(type = &#39;body&#39;)) %&gt;% print(target = &quot;output/5.2.rvg1.pptx&quot;) ## IMPORTANT: ph_with_vg_at is is deprecated. # old syntaxis ph_with_vg_at(code = print(chart_to_plot), left = 1, top = 1, width = 8, height = 6) pptx_obj &lt;- read_pptx() %&gt;% add_slide(layout = &quot;Title and Content&quot;, master = &quot;Office Theme&quot;) %&gt;% ph_with(value = dml(ggobj = chart_to_plot), location = ph_location(left = 1, top = 1, width = 8, height = 6)) %&gt;% print(target = &quot;output/5.2.rvg2.pptx&quot;) # all items on the chart inside the pptx are now editable, just click on any and see # the Shape Format tab in PowerPoint 9.15.2 mschart Package # sample dataframe mydata &lt;- sample_data_list[[&#39;Sample 1&#39;]] %&gt;% group_by(Variable,Response) %&gt;% count() # syntaxis is similar to ggplot2&quot;s aes() with x,y,group my_barchart &lt;- ms_barchart(data = mydata, x = &quot;Variable&quot;, y = &quot;n&quot;, group = &quot;Response&quot;) # to add the object to a powerpoint slide we can use the officer&quot;s native ph_with pptx_obj &lt;- read_pptx() %&gt;% add_slide(layout = &quot;Title and Content&quot;, master = &quot;Office Theme&quot;) %&gt;% ph_with(value = my_barchart, location = ph_location_type(type = &quot;body&quot;)) %&gt;% print(target = &quot;output/5.3.msoffice.pptx&quot;) # if we would open a rvg slide and an ms office slide and click on the slide # for the rvg slide the only menu that appear is shape format # While for the msoffice chart we have now the chart design option with all msoffice functionalities # by using chart_settings() functions one can customise in R the charts 9.16 Word 9.16.1 Word documents Formats share similarities body_add_*() my_doc &lt;- read_docx() %&gt;% body_add_par(value = &quot;My Text&quot;, style = &quot;Normal&quot;) %&gt;% body_add_par(value = &quot;Other Text&quot;, style = &quot;Normal&quot;) %&gt;% body_add_par(value = &quot;Conclusion&quot;, style = &quot;Normal&quot;) %&gt;% print(target = &#39;output/6.1.mydoc.docx&#39;) body_add_break() my_doc &lt;- read_docx() %&gt;% body_add_par(value = &quot;My Text&quot;, style = &quot;Normal&quot;) %&gt;% body_add_break() %&gt;% body_add_par(value = &quot;Conclusion&quot;, style = &quot;Normal&quot;) %&gt;% print(target = &#39;output/6.2.mydoc.docx&#39;) my_format &lt;- fp_text(font.family = &#39;Calibri&#39;, font.size = 14, bold = TRUE, color = &#39;red&#39;) my_text &lt;- ftext(&#39;My dataset is:&#39;, my_format) my_par &lt;- fpar(my_text) doc &lt;- read_docx() %&gt;% body_add_par(value = &quot;Table of content&quot;, style = &quot;heading 1&quot;) %&gt;% body_add_par(value = &quot;&quot;, style = &quot;Normal&quot;) %&gt;% body_add_fpar(my_par, style = &quot;Normal&quot;) %&gt;% #formatted paragraph function body_add_par(value = &quot;&quot;, style = &quot;Normal&quot;) %&gt;% body_add_table(value = head(mtcars)[, 1:4], style = &quot;table_template&quot; ) %&gt;% print(target = &#39;output/6.3.mydoc.docx&#39;) read_docx() %&gt;% styles_info() In conditionalFormatting(), you can specify to which rows and cols the formatting applies. In this example, cols takes v+1 because the first column contains the row names. "],["machine-learning.html", "Chapter 10 Machine Learning 10.1 Concepts and general workflow (training/test) 10.2 Unsupervised learning 10.3 Semisupervised learning 10.4 Supervised learning 10.5 Predictive modeling 10.6 Interpretability 10.7 Cmputer vision 10.8 Other methods and resources", " Chapter 10 Machine Learning 10.1 Concepts and general workflow (training/test) 10.2 Unsupervised learning 10.2.1 Cluster analysis 10.2.2 Factor analysis 10.2.3 Principle components analysis 10.2.4 t-SNE 10.3 Semisupervised learning 10.3.1 PLS regression 10.4 Supervised learning 10.4.1 Regression 10.4.2 K-nearest neighbors 10.4.3 Decision trees 10.4.4 Black boxes 10.4.4.1 Random forests 10.4.4.2 SVMs 10.4.4.3 Neural networks 10.5 Predictive modeling 10.5.1 Predicting sensory profiles from instrumental data 10.5.2 Predicting consumer response from sensory profiles 10.6 Interpretability 10.6.1 LIME 10.6.2 DALEX 10.6.3 IML 10.7 Cmputer vision 10.8 Other methods and resources 10.8.1 Data Prep data &lt;- readr::read_rds(&#39;data/masked_data.rds&#39;) nrows &lt;- max(summary(data$Class)) * 2 data_over &lt;- ROSE::ROSE(Class ~ ., data = data %&gt;% mutate(across(starts_with(&#39;D&#39;), factor, levels = c(0, 1))), N = nrows, seed = 1)$data readr::write_rds(data_over, &#39;data/data_classification.rds&#39;) readxl::read_excel(&#39;data/data_regression.xlsx&#39;) %&gt;% select(-`...1`, -judge, -product, -(steak:V64), -`qtt.drink.(%)`) %&gt;% rename(socio_professional = `socio-professional`) %&gt;% readr::write_rds(&#39;data/data_regression.rds&#39;) 10.8.2 Classification Code library(tidyverse) library(tidymodels) # Load data --------------------------------------------------------------- data &lt;- read_rds(&#39;data/data_classification.rds&#39;) # Inspect the data -------------------------------------------------------- summary(data) data &lt;- data %&gt;% select(-ID) skimr::skim(data) data %&gt;% mutate(across(starts_with(&#39;D&#39;), factor, levels = c(0, 1))) %&gt;% GGally::ggpairs(aes(fill = Class)) # Split data for models --------------------------------------------------- # Set test set aside train_test_split &lt;- initial_split(data) train_test_split train_set &lt;- training(train_test_split) test_set &lt;- testing(train_test_split) # Split set fot cross-validation resampling &lt;- vfold_cv(train_set, 10) resampling # Fit MARS model ---------------------------------------------------------- usemodels::use_earth( Class ~ ., data = train_set ) earth_recipe &lt;- recipe(formula = Class ~ ., data = train_set) %&gt;% step_novel(all_nominal(), -all_outcomes()) %&gt;% step_dummy(all_nominal(), -all_outcomes()) %&gt;% step_zv(all_predictors()) earth_spec &lt;- mars( num_terms = tune(), prod_degree = tune(), prune_method = &quot;none&quot; ) %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;earth&quot;) earth_workflow &lt;- workflow() %&gt;% add_recipe(earth_recipe) %&gt;% add_model(earth_spec) earth_grid &lt;- tidyr::crossing(num_terms = 2 * (1:6), prod_degree = 1:2) earth_grid earth_tune &lt;- tune_grid( earth_workflow, resamples = resampling, # Save predictions for further steps control = control_grid(save_pred = TRUE, verbose = TRUE), # Test parameters on a grid defined above grid = earth_grid ) # Check model performance ------------------------------------------------- earth_tune %&gt;% show_best(n = 10) earth_tune %&gt;% autoplot() earth_predictions &lt;- earth_tune %&gt;% collect_predictions(parameters = select_best(., &#39;roc_auc&#39;)) %&gt;% mutate(model = &quot;MARS&quot;) earth_predictions %&gt;% roc_curve(Class, .pred_A) %&gt;% autoplot() earth_predictions %&gt;% lift_curve(Class, .pred_A) %&gt;% autoplot() earth_predictions %&gt;% pr_curve(Class, .pred_A) %&gt;% autoplot() earth_predictions %&gt;% conf_mat(Class, .pred_class) %&gt;% autoplot() # Fit decision tree ------------------------------------------------------- tree_recipe &lt;- recipe(formula = Class ~ ., data = train_set) %&gt;% step_novel(all_nominal(), -all_outcomes()) %&gt;% step_zv(all_predictors()) tree_spec &lt;- decision_tree( cost_complexity = tune(), tree_depth = tune(), min_n = tune() ) %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;rpart&quot;) tree_workflow &lt;- workflow() %&gt;% add_recipe(tree_recipe) %&gt;% add_model(tree_spec) tree_tune &lt;- tune_grid( tree_workflow, resamples = resampling, # Save predictions for further steps control = control_grid(save_pred = TRUE, verbose = TRUE), # Test 20 random combinations of parameters grid = 20 ) # Check model performance ------------------------------------------------- tree_tune %&gt;% show_best(n = 10) tree_tune %&gt;% autoplot() tree_predictions &lt;- tree_tune %&gt;% collect_predictions(parameters = select_best(., &#39;roc_auc&#39;)) %&gt;% mutate(model = &quot;Decision Tree&quot;) tree_predictions %&gt;% bind_rows(earth_predictions) %&gt;% group_by(model) %&gt;% roc_curve(Class, .pred_A) %&gt;% autoplot() tree_predictions %&gt;% bind_rows(earth_predictions) %&gt;% group_by(model) %&gt;% lift_curve(Class, .pred_A) %&gt;% autoplot() tree_predictions %&gt;% bind_rows(earth_predictions) %&gt;% group_by(model) %&gt;% pr_curve(Class, .pred_A) %&gt;% autoplot() tree_predictions %&gt;% conf_mat(Class, .pred_class) %&gt;% autoplot() # Let&#39;s go with MARS model ------------------------------------------------ final_fit &lt;- earth_workflow %&gt;% finalize_workflow(select_best(earth_tune, &#39;roc_auc&#39;)) %&gt;% last_fit(train_test_split) final_fit %&gt;% collect_metrics() final_fit %&gt;% collect_predictions() %&gt;% roc_curve(Class, .pred_A) %&gt;% autoplot() final_model &lt;- final_fit %&gt;% pluck(&quot;.workflow&quot;, 1) %&gt;% fit(data) final_model %&gt;% pull_workflow_fit() %&gt;% vip::vip() final_model %&gt;% pull_workflow_fit() %&gt;% pluck(&quot;fit&quot;) %&gt;% summary write_rds(final_model, &#39;classification_model.rds&#39;) # Predict something ------------------------------------------------------- model &lt;- read_rds(&#39;classification_model.rds&#39;) new_observation &lt;- tibble( N1 = 1.8, D1 = factor(0), D2 = factor(0), D3 = factor(1), D4 = factor(0), D5 = factor(1), D6 = factor(0), D7 = factor(1), D8 = factor(1), D9 = factor(1), D10 = factor(1), D11 = factor(0) ) predict(model, new_observation, type = &quot;class&quot;) predict(model, new_observation, type = &quot;prob&quot;) 10.8.3 Regression Code library(tidyverse) library(tidymodels) # Load data --------------------------------------------------------------- data &lt;- read_rds(&#39;data/data_regression.rds&#39;) glimpse(data) # Inspect the data -------------------------------------------------------- summary(data) skimr::skim(data) # Split data for models --------------------------------------------------- # Set test set aside train_test_split &lt;- initial_split(data) train_test_split train_set &lt;- training(train_test_split) test_set &lt;- testing(train_test_split) # Split set fot cross-validation resampling &lt;- vfold_cv(train_set, 10) resampling # Fit glmnet model ---------------------------------------------------------- usemodels::use_glmnet( Liking ~ ., data = train_set ) glmnet_recipe &lt;- recipe(formula = Liking ~ ., data = train_set) %&gt;% step_novel(all_nominal(), -all_outcomes()) %&gt;% step_dummy(all_nominal(), -all_outcomes()) %&gt;% step_zv(all_predictors()) %&gt;% step_normalize(all_predictors(), -all_nominal()) glmnet_spec &lt;- linear_reg(penalty = tune(), mixture = tune()) %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;glmnet&quot;) glmnet_workflow &lt;- workflow() %&gt;% add_recipe(glmnet_recipe) %&gt;% add_model(glmnet_spec) glmnet_grid &lt;- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20), mixture = c(0.05, 0.2, 0.4, 0.6, 0.8, 1)) glmnet_tune &lt;- tune_grid( glmnet_workflow, resamples = resampling, # Save predictions for further steps control = control_grid(save_pred = TRUE, verbose = TRUE), # Test parameters on a grid defined above grid = glmnet_grid ) # Check model performance ------------------------------------------------- glmnet_tune %&gt;% show_best(n = 10) glmnet_tune %&gt;% autoplot() glmnet_predictions &lt;- glmnet_tune %&gt;% collect_predictions(parameters = select_best(., &#39;rmse&#39;)) %&gt;% mutate(model = &quot;GLMNet&quot;, .resid = Liking - .pred) glmnet_predictions %&gt;% ggplot(aes(sample = .resid)) + geom_qq() + geom_qq_line() glmnet_predictions %&gt;% ggplot(aes(.pred, Liking)) + geom_point() + geom_abline(slope = 1, intercept = 0) glmnet_predictions %&gt;% ggplot(aes(.pred, .resid)) + geom_point() + geom_hline(yintercept = 0) ggplot(glmnet_predictions, aes(x = .resid)) + geom_histogram(aes(y =..density..), fill = &#39;white&#39;, color = &#39;black&#39;) + stat_function(fun = dnorm, args = list(mean = mean(glmnet_predictions$.resid), sd = sd(glmnet_predictions$.resid)), size = 1) # Fit random forest ------------------------------------------------------- rf_recipe &lt;- recipe(formula = Liking ~ ., data = train_set) %&gt;% step_novel(all_nominal(), -all_outcomes()) %&gt;% step_zv(all_predictors()) rf_spec &lt;- rand_forest( mtry = tune(), min_n = tune(), trees = 50 ) %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;ranger&quot;, importance = &quot;impurity&quot;) rf_workflow &lt;- workflow() %&gt;% add_recipe(rf_recipe) %&gt;% add_model(rf_spec) rf_tune &lt;- tune_grid( rf_workflow, resamples = resampling, # Save predictions for further steps control = control_grid(save_pred = TRUE, verbose = TRUE), # Test 20 random combinations of parameters grid = 20 ) # Check model performance ------------------------------------------------- rf_tune %&gt;% show_best(n = 10) rf_tune %&gt;% autoplot() rf_predictions &lt;- rf_tune %&gt;% collect_predictions(parameters = select_best(., &#39;rmse&#39;)) %&gt;% mutate(model = &quot;Random Forest&quot;, .resid = Liking - .pred) rf_predictions %&gt;% bind_rows(glmnet_predictions) %&gt;% ggplot(aes(sample = .resid)) + geom_qq() + geom_qq_line() + facet_wrap(~model) rf_predictions %&gt;% bind_rows(glmnet_predictions) %&gt;% ggplot(aes(.pred, Liking)) + geom_point() + geom_abline(slope = 1, intercept = 0) + facet_wrap(~model) rf_predictions %&gt;% bind_rows(glmnet_predictions) %&gt;% ggplot(aes(.pred, .resid)) + geom_point() + geom_hline(yintercept = 0) + facet_wrap(~model) rf_predictions %&gt;% ggplot(aes(x = .resid)) + geom_histogram(aes(y =..density..), fill = &#39;white&#39;, color = &#39;black&#39;) + stat_function(fun = dnorm, args = list(mean = mean(rf_predictions$.resid), sd = sd(rf_predictions$.resid)), size = 1) # Let&#39;s go with rf model ------------------------------------------------ final_fit &lt;- glmnet_workflow %&gt;% finalize_workflow(select_best(glmnet_tune, &#39;rmse&#39;)) %&gt;% last_fit(train_test_split) final_fit &lt;- rf_workflow %&gt;% finalize_workflow(select_best(rf_tune, &#39;rmse&#39;)) %&gt;% last_fit(train_test_split) final_fit %&gt;% collect_metrics() final_fit %&gt;% collect_predictions() %&gt;% mutate(.resid = Liking - .pred) %&gt;% ggplot(aes(sample = .resid)) + geom_qq() + geom_qq_line() final_model &lt;- final_fit %&gt;% pluck(&quot;.workflow&quot;, 1) %&gt;% fit(data) final_model %&gt;% pull_workflow_fit() %&gt;% vip::vip() # final_model %&gt;% # broom::tidy() %&gt;% # filter(estimate != 0) write_rds(final_model, &#39;regression_model.rds&#39;) # Predict something ------------------------------------------------------- model &lt;- read_rds(&#39;regression_model.rds&#39;) new_observations &lt;- data[1:2,] new_observations predict(model, new_observations) "],["text-analysis.html", "Chapter 11 Text Analysis 11.1 Data import 11.2 Analysis 11.3 RAW MATERIAL", " Chapter 11 Text Analysis 11.1 Data import 11.1.1 Data sources 11.1.2 Tokenizing 11.1.3 Lemmatization, stemming, and stop word removal 11.2 Analysis 11.2.1 Frequency counts and summary statistics 11.2.2 Word clouds 11.2.3 Contrast plots 11.2.4 Sentiment analysis 11.2.5 Bigrams and word graphs 11.3 RAW MATERIAL Introduction to {tidytext} and {Xplortext} 11.3.1 Statistical entities What are we considering as statistical entities? documents sentences words cleaned words Depends on objectives of study and how data are being collected: directly from consumers in a CLT (directed questions) analysis of social media (e.g. twitter) web-scrapping from website Discussion around CATA as a simplified version of text analysis 11.3.1.1 Notion of tokenization 11.3.1.2 Cleaning the data Notions of lemmatization, stemming, and stopwords removal grouping words removing stopwords tf-idf 11.3.2 Analysis of Frequencies and term-frequency document 11.3.2.1 Contingency table Presentation of the tf/contingency table 11.3.2.2 wordclouds {ggwordclouds} 11.3.2.3 Correspondence Analysis {FactoMineR} and {Xplortext} 11.3.3 Futher Analysis of the words 11.3.3.1 Sentiment Analysis Sentiment analysis and its relationship to hedonic statement Introduction to free-JAR? 11.3.3.2 Bi-grams and N-grams Presentation of graph-theory applied to text mining 11.3.3.3 Machine learning Introduction to machine learning associated to text mining "],["next-steps.html", "Chapter 12 Whats Next? 12.1 Shiny 12.2 Graph Databases 12.3 Sensory Analysis in R 12.4 Learning Resources 12.5 Python", " Chapter 12 Whats Next? 12.1 Shiny 12.2 Graph Databases 12.3 Sensory Analysis in R 12.4 Learning Resources 12.5 Python "],["references.html", "References", " References Bryan, J. (2018). Happy git and github for the useR. GitHub. Available from https://happygitwithr.com/ Gillespie, C., &amp; Lovelace, R. (2016). Efficient R programming: A practical guide to smarter programming. \" OReilly Media, Inc.. Grolemund, G. Getting Started with R. Rstudio Support. Available from https://support.rstudio.com/hc/en-us/articles/201141096-Getting-Started-with-R Norman, D. (2013). The design of everyday things: Revised and expanded edition. Basic books. Peng, R., Caffo, B., &amp; Leek, J. Data Science Specialization [Online course]. Available from https://www.coursera.org/specializations/jhu-data-science Peng, R., Caffo, B., &amp; Leek, J. Executive data science specialization [Online course]. Available from https://www.coursera.org/specializations/executive-data-science Wickham, H., &amp; Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data. OReilly Media, Inc.\". Wickham, H. (2016). ggplot2: elegant graphics for data analysis. Springer. Zuur, A., Ieno, E. N., &amp; Meesters, E. (2009). A Beginners Guide to R. Springer Science &amp; Business Media. Breiman, L. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199-231. Cleveland, W. S. (2001). Data science: an action plan for expanding the technical areas of the field of statistics. International statistical review, 69(1), 21-26. Fayyad, U., Piatetsky-Shapiro, G., &amp; Smyth, P. (1996). From data mining to knowledge discovery in databases. AI magazine, 17(3), 37-37. Naur, P. (1966). The science of datalogy. Communications of the ACM, 9(7), 485. Tukey, J. W. (1962). The future of data analysis. The annals of mathematical statistics, 33(1), 1-67. Tukey, J. W. (1977). Exploratory data analysis. Reading, Mass: Addison-Wesley Pub. Co. Wu, C. F. J. (1997) Statistics = Data Science? Lecture notes available online at http://www2.isye.gatech.edu/~jeffwu/presentations/datascience.pdf "]]
