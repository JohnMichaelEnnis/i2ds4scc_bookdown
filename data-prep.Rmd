```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```
# Data Preparation {#data-prep}

To analyze data, one need *data*. If this data is already available in R, then the analysis can be performed directly. However, in much cases, the data is stored outside the R environment, and needs to be imported. 

## Data Importation

In practice, the data might be stored in as many format as one can imagine, whether it ends up being a fairly common solution (.txt file, .csv file, or .xls/.xlsx file), or software specific (e.g. Stata, SPSS, etc.).
Since it is very common to store the data in Excel spreadsheets (.xlsx) due to its simplicity, the emphasis is on this solution. Fortunately, most generalities presented for Excel files also apply to other formats through `base::read.table()` for .txt files,  `base::read.csv()` and `base::read.csv2()` for .csv files, or through the `{read}` package (which is part of the `{tidyverse}`).

For other (less common) formats, the reader can find packages that would allow importing their files into R. Particular interest can be given to the package `{rio}` (*rio* stands for *R* *I*nput and *O*utput) which provides an easy solution that 1. can handle a large variety of files, 2. can actually guess the type of file it is, and 3. provides tools to import, export, and convert almost any type of data format, including .csv, .xls and .xlsx, or data from other statistical software such as SAS (.sas7bdat and .xpt), SPSS (.sav and .por), or Stata (.dta). As an alternative, the package `{foreign}` provides functions that allow importing data stored from other statistical software (incl. Minitab, S, SAS, Stata, SPSS, etc.)

Although Excel is most likely one of the most popular way of storing data, there are no `{base}` functions that allow importing such files easily. Fortunately, many packages have been developed in that purpose, including `{XLConnect}`, `{xlsx}`, `{gdata}`, and `{readxl}`. Due to its convenience and speed of execution, we will be using `{readxl}` here.

### Importing Structured Excel File

First, let's import the *Sensory Profile.xlsx* workbook using the `readxl::read_xlsx()` file, by informing as parameter the location of the file (informed in `file_path` using the package `{here}`) and the `sheet` we want to read from.

This file is called _structured_ as all the relevant information is already stored in the same sheet in a structured way. In other words, no decoding is required here, and there are no 'unexpected' rows or columns (e.g. empty lines, or lines with additional information regarding the data but that is not data):

 - The first row within the *Data* sheet of *Sensory Profile.xlsx* contains the headers,  
 - From the second row onwards, only data is being stored.
 
Since this data will be used for some analyses, it is assigned data to an R object called `sensory`.

```{r}
library(here)
file_path <- here("data","Sensory Profile.xlsx") 

library(readxl)
sensory <- read_xlsx(file_path, sheet="Data")
```

To ensure that the importation went well, we print `sensory` to see how it looks like. Since `{readxl}` has been developed by Hadley Wickham and colleagues, its functions follow the `{tidyverse}` principles and the dataset thus imported is a `tibble`. Let's take advantage of the printing properties of a `tibble` to evaluate `sensory`:

```{r}
sensory
```

`sensory` is a tibble with 99 rows and 35 columns that includes the `Judge` information (first column, defined as character), the `Product` information (second and third columns, defined as character), and the sensory attributes (fourth column onward, defined as numerical or `dbl`).

Additionally, we can also print a `summary()` of `sensory` to get some extra information regarding the data (such as the minimum, maximum, mean and median for each numerical variable)"

```{r}
summary(sensory)
```

At last, we can list the structure of the dataset through the `str()` function.

```{r}
str(sensory)
```

This function provides an overview of the first element of each variables (and the format they are in) as a list, and allows the user to have a first scan to eventually detect *errors* or *mishaps* during the importation.

Here, the data has been imported as expected.

### Importing Unstructured Excel File

In some cases, the dataset is not so well organized/structured, and may need to be *decoded*. This is the case for the workbook entitled *TFEQ.xlsx*. For this file:

 - The variables' name have been coded and their corresponding names (together with some other valuable information we will be using in the next chapter) are stored in a different sheet entitled _Variables_;
 - The different levels of each variable (including their code and corresponding names) are stored in another sheet entitled _Levels_.

To import and decode this dataset, multiple steps are required:

 - Import the variables' name only;
 - Import the information regarding the levels;
 - Import the dataset without the first line of header, but by providing the correct names obtained in the first step;
 - Decode each question (when needed) by replacing the numerical code by their corresponding labels.

Let's start with importing the variables' names from *TFEQ.xlsx* (sheet *Variables*)

```{r}
file_path <- here("data","TFEQ.xlsx") 

var_names <- read_xlsx(file_path, sheet="Variables")
var_names
```

In a similar way, let's import the information related to the levels of each variable, stored in the *Levels* sheet.
A deeper look at the *Levels* sheet shows that only the coded names of the variables are available. In order to include the final names, `var_names` is joined (using `inner_join`).

```{r}
library(tidyverse)
var_labels <- read_xlsx(file_path, sheet="Levels") %>% 
  inner_join(dplyr::select(var_names, Code, Name), by=c(Question="Code"))

var_labels
```


**Note**: In some cases, this information is directly available in the dataset as sub-header: A solution is then to import the first rows of the dataset that contain this information using the parameter `n_max` from `readxl::read_xlsx``. For each variable (when information is available), store that information as a list of tables that contains the code and their corresponding label.

**THIS SECTION BELOW MIGHT NEED TO GET PASSED ON TO THE EXERCISE**

Since most likely this system of coding follow a fixed pattern, we strongly recommend the use of `{tidytext}` and its function `unnest_tokens()`.
For example, let's imagine that the our information is structured as *code1=label1,code2=label2,...* (e.g. *0=No,1=Yes*). In such case, first use `unnest_tokens()` to split this string by ','. This creates a tibble with as many rows as there are *code=label* and one column. Next, split this column into two columns using `separate()` and `sep="="`.

**(PREVIOUS PART) TO BE GIVEN AS AN EXAMPLE/EXERCISE**

Finally, we import the dataset (*Data*) by substituting the coded names with their real names.
To do so, we skip reading the first line (`skip=1`) that contains the coded names, and force the column names to be defined by `Name` from `var_names` (after ensuring that the names' sequence perfectly match across the two tables!).

```{r}
TFEQ_data <- read_xlsx(file_path, sheet="Data", col_names=var_names$Name, skip=1)
summary(TFEQ_data)
```

The data has now the proper header, however each variable is still coded numerically. 

## Data Inspection

### Data Type

In R, the variables can be of different types, going from numerical to nominal to binary etc. This section aims in presenting the most common types (and their properties) used in sensory and consumer studies, and in showing how to transform a variable from one type to another.

Remember that when your dataset is a tibble (as is the case here), the type of each variable is provided as sub-header when printed on screen. This eases the work of the analyst as the variables' type can be assessed at any moment. 

In case the dataset is not in a tibble, the use of the `str()` function used previously becomes handy as it provides this information.

In sensory and consumer research, the four most common types are:

 - Numerical (incl. integer or `int`, decimal or `dcl`,  and double or `dbl`);
 - Logical or `lgl`;
 - Character or `char`;
 - Factor or `fct`.

R still has plenty of other types, for more information please visit: https://tibble.tidyverse.org/articles/types.html

#### Numerical Data

Since a large proportion of the research done is quantitative, it is no surprise that our dataset are often dominated with numerical variables. In practice, numerical data includes integer (non-fractional number, e.g. 1, 2, -16, etc.), or decimal value (or double, e.g. 1.6, 2.333333, -3.2 etc.).
By default, when reading data from an external file, R converts any numerical variables to integer unless decimal points are detected, in which case it is converted into double.

**Do we want to show how to format R wrt the number of decimals? (e.g. options(digits=2))**

#### Binary Data

Another common type that seem to be numerical in appearance, but that has additional properties is the binary type. 
Binary data is data that takes two possible values (`TRUE` or `FALSE`), and are often the results of a *test* (e.g. is `x>3`? Or is `MyVar` numerical?). A typical example of binary data in sensory and consumer research is data collected through Check-All-That-Apply (CATA) questionnaires.

Note: Intrinsically, binary data is *numerical*, TRUE being assimilated to 1, FALSE to 0. If multiple tests are being performed, it is possible to sum the number of tests that pass using the `sum()` function, as shown in the simple example below:


```{r}
set.seed(123456)
# Generating 10 random values between 1 and 10 using the uniform distribution
x <- runif(10, 1, 10)
x

# Test whether the values generated are strictly larger than 5
test <- x>5
test

# Counting the number of values strictly larger than 5
sum(test)
```

#### Nominal Data

Nominal data is any data that is not numerical. In most cases, nominal data are defined through text, or strings. It can appear in some situations that nominal variables are still defined with numbers although they do not have a numerical meaning. This is for instance the case when the respondents or samples are identified through numerical codes: In that case, it is clear that respondent 2 is not twice larger than respondent 1 for instance. But since the software cannot guess that those numbers are *identifiers* rather than *numbers*, the variables should be declared as nominal. The procedure explaining how to convert the type of the variables will be explained in the next section. 

For nominal data, two particular types of data are of interest: 

 - Character or `char`;
 - Factor or `fct`.
  
Variables defined as character or factor take strings as input. However, these two types differ in terms of structure of their levels: 

 - For `character`, there are no particular structure, and the variables can take any values (e.g. open-ended question);
 - For `factor`, the inputs of the variables are structured into `levels`.
 
To evaluate the number of levels, different procedure are required:

 - For `character`, one should count the number of unique element using `length()` and `unique()`;
 - For `factor`, the levels and the number of levels are direcly provided by `levels()` and `nlevels()`.
 
Let's compare a variable set as `factor` and `character` by using the `Judge` column from `TFEQ_data`:

```{r}
example <- TFEQ_data %>% 
  dplyr::select(Judge) %>% 
  mutate(Judge_fct = as.factor(Judge))

print("Summary:")
summary(example)

print("As Character:")
unique(example$Judge)
length(unique(example$Judge))

print("As Factor:")
levels(example$Judge_fct)
nlevels(example$Judge_fct)
```

Although `Judge` and `Judge_fct` look the same, they are structurally different, and those differences play an important role that one should consider when running certain analyses, or building tables and graphs.

When set as `character`, the number of levels of a variable is directly read from the data, and its levels' order would either match the way they appear in the data, or are ordered alphabetically. This means that any data collected using a structured scale will lose its natural order. 
When set as `factor`, the number and order of the factor levels are informed, and does not depend on the data itself: If a level has never been selected, or if certain groups have been filtered, this information is still present in the data. 

To illustrate this, let's re-arrange the levels from `Judge_fct` by ordering them numerically in such a way `J2` follows `J1` rather than `J10`.

```{r}
judge <- str_sort(levels(example$Judge_fct),numeric=TRUE)
judge
levels(example$Judge_fct) <- judge
```

Now the levels are sorted, let's 'remove' some respondents by only keeping the 20 first ones (J1 to J20, as J18 does not exist), and re-run the previous code:

```{r}
example <- TFEQ_data %>% 
  dplyr::select(Judge) %>% 
  mutate(Judge_fct = as.factor(Judge)) %>% 
  filter(Judge %in% paste0("J",1:20))

dim(example)

print("As Character:")
unique(example$Judge)
length(unique(example$Judge))

print("As Factor:")
levels(example$Judge_fct)
nlevels(example$Judge_fct)
```

After filtering some respondents, it can be noticed that the variable set as character only contains 19 elements, whereas the column set as factor still contains the 107 respondents (most of them not having any recordings). This property can be seen as an advantage or a disadvantage depending on the situation:

 - For frequencies, it may be relevant to remember all the options, including the ones that may never be selected, and to order the results logically (use of `factor`).
 - For hypothesis testing (e.g. ANOVA) on subset of data (e.g. the data being split by gender), the `Judge` variable set as `character` would have the correct number of degrees of freedom (18 in our example) whereas the variable set as factor would use 106 degrees of freedom in all cases!

The latter point is particularly critical since the analysis is incorrect and will either return an error or worse return erroneous results!

Last but not least, variables defined as factor allow having their levels being renamed (and eventually combined) very easily. 
Let's consider the `Living area` variable from `TFEQ_data` as example. From the original excel file, it can be seen that it has three levels, `1` corresponding to *urban area*, `2` to *rurban area*, and `3` to *rural area*.
Let's start by renaming this variable accordingly:


```{r}
example = TFEQ_data %>% 
  mutate(Area = factor(`Living area`, levels=c(1,2,3), labels=c("Urban", "Rurban", "Rural")))

levels(example$Area)
nlevels(example$Area)

table(example$`Living area`, example$Area)
```


As can be seen, the variable `Area` is the factor version (including its labels) of `Living area`.
If we would also consider that `Rurban` should be combined with `Rural`, and that `Rural` should appear before `Urban`, we can simply modify the code as such:


```{r}
example = TFEQ_data %>% 
  mutate(Area = factor(`Living area`, levels=c(2,3,1), labels=c("Rural", "Rural", "Urban")))

levels(example$Area)
nlevels(example$Area)

table(example$`Living area`, example$Area)
```

This approach of renaming and re-ordering factor levels is very important as it simplifies the readability of tables and figures.
Some other transformations can be applied to factors thanks to the `{forcats}` package. Amongst other relevant functions, particular attention can be given to:

 - `fct_reorder`/`fct_reorder2` and `fct_relevel` which reorder the levels of a factor;
 - `fct_recode` which helps recoding a factor (as an alternative to `factor` used in the previous example);
 - `fct_collapse` and `fct_lump` which allows aggregating different levels together (`fct_lump` regroups automatically all the rare levels).

Although it hasn't been done here, manipulating strings is also possible. To do so, the `{stringr}` package provides a lot of of interesting functions, such as:

 - `str_to_upper`/`str_to_lower` which automatically convert strings to uppercase or lowercase;
 - `str_c`, `str_sub` which combine or subset strings;
 - `str_trim` and `str_squish` which help remove white spaces;
 - `str_extract`, `str_replace`, `str_split` to extract, replace, or split strings or part of the strings.
 
 
### Changing the type of a variable

Transforming the type of variables using `mutate()`: 

 - from logical to numerical;
 - from numerical to character/factor;
 - from character/factor to numerical

## Data Organization

Presentation of the different shapes of the tables based on objectives

## Data Manipulation

### Type of table

matrix, data frame, and tibble. 

how to check the type? `class()`
how to test it? `is.data.frame()`, `is.matrix()`, `is_tibble()`
how to convert it to another format? (see below)

Note on `{FactoMineR}` and `{SensoMineR}` which require data frames or matrix (not tibble) so introduction to `column_to_rownames()` and `rownames_to_columns()` as well as `as.data.frame()` and `as_tibble()`.

### Data organisation

`select()`, `filter()`, `arrange()`
`mutate()` and `transmute()`

### Data re-structuration

`pivot_wider()` and `pivot_longer()`
`full_join()`, `inner_join()`, `left_join()` and `right_join()`

`unnest_token()` from **{tidytext}**

## Cleaning and Quality Assessment

### Renaming

renaming columns using `rename()` or `select()`
renaming elements using `factor()` and **{forcats}**

### Recoding

Example of recoding (could be renaming, or replacing NAs, etc.) by combining `mutate()` and `ifelse()`

### Handling Missing Values

Removing and replacing NAs

### Quality Assessment

Graphics?